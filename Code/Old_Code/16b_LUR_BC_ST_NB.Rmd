---
title: "Fitting the ST model for the LUR"
author: "Sheena Martenies"
date: "06/11/2020"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = F)
```

# Meeting Notes:
After meeting with Josh Keller (Statistics, CSU), we decided to try to implement the modeling framework used by the MESA Air team (Keller et al., 2015). This was implemented using the SpatioTemporal package in R.

The original paper used PLS to select the spatial covariates. That will be the eventual approach for this analysis, but for now, I'm going to use the covariates selected by the LASSO model (see 16a_LUR_BC_average.R)

**Update 03.18.20**: I was receiving an error when trying to use the estimate.STmodel() function from the SpatioTemporal package. I emailed Josh Keller for advice, and he responded with some suggestions. I've pasted his email response at the end of this script for safe keeping

Basically, I'm going to start with the following:

- Simple model (i.e., only one time trend)
- iid covariance structure

**Update 03.24.20**: Sheryl and I met with Josh Keller again today to go over the preliminary version of the model and talk about next steps. Here are some of the take-aways from the meeting:

- We should add the entirety of the BC monitoring data at the central site to help establish long-term time trends
- We need to add degrees of freedom to the time trends. Josh recommended 4 per year (but we can also try 8, 12, etc.). Time trends will become more obvious with more data
- I need to use the PM and NO2 monitoring data to establish temporal trends. These are different from the ST predictors! Start by plotting the time trends of PM, temp, and NO2. We might want to use PM for the temporal trends and NO2 and Temp as ST predictors (other combinations might work too, such as adding PM to the ST predictors). Don't use BC as a ST predictor
    - When looking at the temporal trends, use ```createSTdata```
    - Need to standardize PM and NO2
    - When ready to create the ```STdata``` object for model fitting, assign the temporal trend data set to the ```$trend``` slot of the ```STdata``` object
- We also want to run a model without ST predictors as a comparison
- Make the LUR function the same for the intercept and the time trends 
- Rather than set ```nugget = ~type```, set ```nugget = T```
- Change from the ```iid``` covariance structure to the ```exp``` structure
- Run a number of initial conditions to try to find the right minimum
    - For nuggets, set them to between -1 and -6
    - For ranges, set them to between 0 and 4
- Cross validation!
    - For distributed sites, use 10-fold CV
    - For central site, use LOOCV
- There is a trade-off for the number of basis functions selected. For now, I'm going to stick with one, and then we can see if more make sense later

**Update 04.20.20**: So far, it looks like I've been able to fit the model using the long-term PM data (scaled) for the temporal trends. There are still some outstanding things I need to work on:

- [] Adding additional ST covariates (_e.g._, smoke days, temperature)
- [] Using PLS to identify spatial covariates
- [] Figure out how to predict to the full grid using the CV model 

I'm going to stick with the LASSO-selected spatial predictors for now. In the future, I'll be implementing PLS.

**Update 04.24.20**: Josh and I met to discuss some challenges with the model this far. These issues included:

- User error! I never actually updated the time trend using the PM data-- need to fix this bit of the code
- for the PM STdata object, manually set the DF to 4 per year (i.e., set `df = 12` rather than use dynamic code)
- Log-transformation might not be necessary for this data set. The CV plots show a lot of outliers at the low end of the range. I need to look at histograms for each distribution (with and without transformation) to see
- Campaign 4 might be an issue here. The time series of BC measurements show a lot of variability in BC. These crappy data might be influencing the model to the point where we cannot fit something well. We might need to drop Campaign 4 from the model. This might not be too big of an issue given that the model can account for time trends. I'm going to explore this now.

This version of the document contains the following changes:

- I added the details about the data set from previous scripts
- I dropped Campaign 4 data. The CV plot looks like this version of the model is performing better
- I used both log-transformed BC and BC in the original units until I got to the CV step.
- I was able to generate time trends using the PM and BC data, but I'm not sure they are an improvement

**Update 05.05.20**: Some updates in this version of the document based on my most recent conversation with Josh:

- Sticking with the log-transformed BC measurements
- Going to try NO~2~ to identify long-term temporal trends instead of PM~2.5~
- Try an "intermediate" model without spatial smoothing in the spatial predictors but with spatial smoothing in the time trend (i.e., set ```cov.beta == c("iid")``` and ```cov.nu == "exp"```)

An important note to keep in mind is that even if dropping the spatial smoothing helps with these predictors, that might change once we shift to using PLS to select spatial covariates. The additional model used here will just help guide final model decisions.

Main take-aways from this version of the document:

- It looks like the model with smoothing in the temporal trend but not the spatial predictors (i.e., Model 2) performed well
- Not sure if Model 3 adds value. It'll be helpful to get Josh's and Sheryl's thoughts 
- I think I can use the NO2 data to estimate the temporal trends, and it looks like it works best with 2 basis functions
- I'm unclear about how to use these new temporal trends in the model. When trying to create the ST model object with the new trends, I got an error about subscripts being out of bounds. Do I need to add the NO2 and BC data to the ```bc_obs``` object in the ```denver.data2.2``` object? This will need sussing out!

Some next steps include:

- Fixing the issue described above to see if we can get a model that has longer-term data
- PLS vs. LASSO for choosing spatial predictors

**Update 05.12.20**: Josh sent some advice about fitting the model with the time trends using the NO2 data. Here I'm creating a new version of bc_st_no2 that goes as far back as the temporal trends (i.e., goes back to 2009). At first it looked like we would need to use two temporal trends, so that's how I fit the models. I also include a summary table of model diagnostics at the end.

Some additional updates:

- Added another intermediate model that uses ans exp covariance structure for the beta0 field and the errors but iid for the beta1 field
- Added the new basis functions (Models 2.X), but had some trouble fitting all the models (see below)
- Tried using just one basis function (even though there was residual autocorrelation) just to have a comparison
- Added models that used two ST predictors (smoke days as a binary variable and NO2 as a continuous variable). These models used the two basis functions

Overall, I'm not seeing much variability in the R^2^ or RSME values. However, there is some difference in whether all of the CV models converge, so perhaps that's a differentiating criterion. 

In my (mostly uninformed) opinion, Model 4.2 is looking pretty good to mode forward with. It uses spatial smoothing in the error term, has long-term trend data with two basis functions thanks to the NO2 and BC monitors, it includes both NO2 and smoke days as spatiotemporal predictors, all of the CV models converged, and the R2 value is above 0.8.

```{r, echo = F}
# library(devtools)
# install_version("SpatioTemporal", version = "1.1.9.1", repos = "https://cran.r-project.org")

library(SpatioTemporal)
library(numDeriv)
library(Matrix)
library(plotrix)
library(maps)
```

```{r, echo = F, include = F, warnings = F}
library(sf)
library(gstat)
library(sp)
library(raster)
library(ggplot2)
library(ggmap)
library(ggsn)
library(ggthemes)
library(extrafont)
library(GGally)
library(ggcorrplot)
library(stringr)
library(tidyverse)
library(lubridate)
library(readxl)
library(viridis)
library(caret)
library(knitr)
library(kableExtra)

#' For ggplots
# loadfonts(device = "win")

simple_theme <- theme(
  #aspect.ratio = 1,
  text  = element_text(size = 12, color = 'black'),
  panel.spacing.y = unit(0,"cm"),
  panel.spacing.x = unit(0.25, "lines"),
  panel.grid.minor = element_line(color = "grey90"),
  panel.grid.major = element_line(color = "grey90"),
  panel.border=element_rect(fill = NA),
  panel.background=element_blank(),
  axis.ticks = element_line(colour = "black"),
  axis.text = element_text(color = "black", size=10),
  # legend.position = c(0.1,0.1),
  plot.margin=grid::unit(c(0,0,0,0), "mm"),
  legend.key = element_blank()
)

options(scipen = 9999) #avoid scientific notation

albers <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
ll_nad83 <- "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
ll_wgs84 <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
```

# Exploring the LUR data set
```{r, include = F, echo = F}

#' Read in the dataset and subset to just the outdoor filters
#' Don't worry about the "central site" filters right now
#' Define which variable we want to use-- BC calibrated using Deming regression

data_name <- "Combined_Filter_Data_AEA.csv"
lur_data <- read_csv(here::here("Data", data_name)) %>% 
  filter(!is.na(lon)) %>% 
  filter(is.na(is_blank) | is_blank == 0) %>% 
  filter(indoor == 0) %>% 
  #filter(campaign %in% paste0("Campaign", c(1, 2, 3))) %>%
  filter(bc_ug_m3_dem > 0) %>% 
  filter(is.na(below_lod) | below_lod == 0) %>% 
  filter(is.na(low_volume_flag) | low_volume_flag == 0) %>% 
  filter(is.na(flow_rate_flag) | flow_rate_flag == 0) %>% 
  filter(is.na(negative_pm_mass) | negative_pm_mass == 0) %>% 
  filter(is.na(potential_contamination) | potential_contamination == 0) %>% 
  filter(is.na(bc_mass_ug_corrected) | bc_mass_ug_corrected >= 0.49) %>% 
  mutate(LoggedRuntime = as.numeric(LoggedRuntime)) %>% 
  mutate(StartDateLocal = as.Date(StartDateLocal, format = "%m/%d/%Y"),
         EndDateLocal = as.Date(EndDateLocal, format = "%m/%d/%Y")) 

#' Select a "calibrated" version of the data
#' For now, go with Deming regression-- accounts for variability in the
#' monitor and the UPAS data
lur_data <- lur_data %>% 
  rename("pm_ug_m3_raw" = "pm_ug_m3") %>% 
  rename("pm_ug_m3" = "pm_ug_m3_dem") %>% 
  rename("bc_ug_m3_raw" = "bc_ug_m3") %>% 
  rename("bc_ug_m3" = "bc_ug_m3_dem")

#' Separate central site data from distributed site data
aqs_data <- filter(lur_data, filter_id == "080310027")

lur_data <- lur_data %>% filter(filter_id != "080310027")

#' Add a unique site ID
lur_data <- mutate(lur_data, site_id_lonlat = paste(lon, lat, sep = "_"))

ids <- select(lur_data, site_id_lonlat) %>% 
  distinct() %>% 
  mutate(site_id = seq_along(site_id_lonlat))
write_csv(ids, here::here("Data", "Monitor_Site_IDs.csv"))

lur_data <- left_join(lur_data, ids, by = "site_id_lonlat") %>% 
  arrange(StartDateLocal, site_id)

#' How much data do we have?
nrow_lur <- nrow(lur_data)
n_campaigns <- length(unique(lur_data$campaign))
n_sites <- length(unique(lur_data$site_id))

dates <- lur_data %>% 
  select(StartDateLocal, EndDateLocal) %>% 
  arrange(StartDateLocal)
earliest <- dates$StartDateLocal[1]
latest <- dates$EndDateLocal[nrow(dates)]
date_seq <- seq.Date(earliest, latest, by = "day")

date_df <- lur_data %>% 
  select(campaign, StartDateLocal, EndDateLocal) %>% 
  arrange(StartDateLocal) %>% 
  group_by(campaign) %>% 
  summarize(Start_Date = StartDateLocal[1],
            End_Date = EndDateLocal[length(EndDateLocal)])

#' When were samples collected?
site_tab2 <- lur_data %>% 
  select(site_id, campaign) %>% 
  group_by(site_id) %>% 
  count(campaign) %>% 
  pivot_wider(id_cols = site_id, names_from = campaign, values_from = n) %>% 
  mutate(total = sum(Campaign1, Campaign2, Campaign3, Campaign4, na.rm = T))

#' How many unique sites were included in each campaign?
camp_tab <- select(lur_data, campaign, site_id) %>% 
  group_by(campaign) %>% 
  summarize(unique_sites = length(unique(site_id)))
```

```{r, include = F, echo = F}
counties <- c("001", "005", "013", "014", "031", "059")

#' PM2.5 concentrations
pm_data <- read_csv(here::here("Data", "Monitor_PM_Data_AEA.csv")) %>% 
  filter(!is.na(Arithmetic_Mean)) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(week = isoweek(Date_Local),
         month = month(Date_Local),
         year = year(Date_Local)) %>% 
  mutate(month_yr = paste(month, year, sep="_"),
         start_iso_week = isoweek(as.Date(Date_Local))) %>%
  filter(Sample_Duration != "1 HOUR") %>% 
  filter(County_Code %in% counties) %>% 
  filter(Date_Local %in% date_seq)

#' Black carbon
bc_data <- read_csv(here::here("Data", "Monitor_BC_Data_AEA.csv")) %>% 
  filter(!is.na(Arithmetic_Mean)) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(week = isoweek(Date_Local),
         month = month(Date_Local),
         year = year(Date_Local)) %>% 
  mutate(month_yr = paste(month, year, sep="_"),
         start_iso_week = isoweek(as.Date(Date_Local))) %>%
  filter(County_Code %in% counties) %>% 
  filter(Date_Local %in% date_seq)

#' NO2
no2_data <- read_csv(here::here("Data", "Monitor_NO2_Data_AEA.csv")) %>% 
  filter(!is.na(Arithmetic_Mean)) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(week = isoweek(Date_Local),
         month = month(Date_Local),
         year = year(Date_Local)) %>% 
  mutate(month_yr = paste(month, year, sep="_"),
         start_iso_week = isoweek(as.Date(Date_Local))) %>%
  filter(County_Code %in% counties) %>% 
  filter(Date_Local %in% date_seq)
plot(st_geometry(no2_data))

#' Temperature
temp_data <- read_csv(here::here("Data", "Monitor_TEMP_Data_AEA.csv")) %>% 
  filter(!is.na(Arithmetic_Mean)) %>% 
  st_as_sf(wkt = "WKT", crs = albers)%>% 
  mutate(week = isoweek(Date_Local),
         month = month(Date_Local),
         year = year(Date_Local)) %>% 
  mutate(month_yr = paste(month, year, sep="_"),
         start_iso_week = isoweek(as.Date(Date_Local))) %>%
  filter(County_Code %in% counties) %>% 
  filter(Date_Local %in% date_seq)
```

## The sampling framework

In total, we have `r nrow_lur` filters collected across `r n_campaigns` sampling campaigns. We collected data at `r n_sites` distributed sites and `r nrow(aqs_data)` EPA monitoring sites. Due to a variety of logtistical challenges, not all sites were available for all campaigns. The number of distributed site sampling locations for Campaigns 1, 2, 3, and 4 were `r camp_tab$unique_sites[1]`, `r camp_tab$unique_sites[2]`, `r camp_tab$unique_sites[3]`, and `r camp_tab$unique_sites[4]`, respectively.

Out goal was to obtain at least 6 weekly integrated samples at each location. The number of samples collected at each distriubted monitoring site varied from `r min(site_tab2$total)` to `r max(site_tab2$total)`. 

The campaigns roughly covered the early and late summer, fall, and winter seasons. We are lacking coverage during the spring season. Due to some interesting relationships between our co-located distributed site monitor data and the central BC monitor data during the winter, we are conducting a follow-up winter/spring campaign right now.

UPDATE: Due to logistical challenges associated with the COVID-19 pandemic, we were unable to complete our follow-up campaign.

```{r echo = F, include = T}
kable(date_df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))%>%
  kable_styling(fixed_thead = T)
```

On average, weekly sampling periods lasted `r round(mean(lur_data$LoggedRuntime, na.rm=T)/24,2)` days and ranged from `r round((min(lur_data$LoggedRuntime, na.rm = T)/24), 2)` to `r round(max(lur_data$LoggedRuntime)/24,2)`. 90% of samples collected has runtimes exceeding `r round(quantile(lur_data$LoggedRuntime, probs = 0.10, na.rm = T)/24,2)` days.

```{r, include = T, echo = F}
ggplot(lur_data) +
  geom_histogram(aes(x = LoggedRuntime/24)) +
  xlab("Sample runtime (days)") +
  simple_theme
```

We also have daily PM~2.5~ data from `r length(unique(pm_data$monitor_id))` central site monitors, temperature data from `r length(unique(temp_data$monitor_id))`, and NO~2~ data from `r length(unique(no2_data$monitor_id))` central site monitors in the 6-county Denver metro area.

###Table summarizing the number of weekly-integrated filter samples collected at each sampling location for each campaign

```{r echo = F, include = T}
kable(site_tab2) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))%>%
  kable_styling(fixed_thead = T)
```

###Plot of when distributed site monitors were deployed
```{r echo = F, include = T}
ggplot(lur_data) +
  geom_segment(aes(x = StartDateLocal, xend = EndDateLocal, 
                   y = site_id, yend = site_id,
                   col = as.factor(campaign))) +
  scale_color_viridis(discrete = T, name = "Campaign") +
  scale_x_date(date_breaks = "2 months") +
  xlab("Sampling duration") + ylab("Distributed site ID") +
  simple_theme
```

###Our sampling locations cover the entire Denver metro area within the 470 loop.
``` {r, echo = F, include = F}
data_name <- "Combined_Filter_Data_AEA.csv"
lur_data_sf <- read_csv(here::here("Data", data_name)) %>%
  filter(!is.na(lon)) %>%
  mutate(site_id_lonlat = paste(lon, lat, sep = "_")) %>%
  left_join(ids, by = "site_id_lonlat") %>%
  st_as_sf(coords = c('lon', 'lat'), crs = ll_wgs84) %>%
  st_transform(crs = albers) %>%
  filter(indoor == 0) %>%
  filter(bc_ug_m3_dem > 0) %>% 
  filter(is.na(below_lod) | below_lod == 0) %>% 
  filter(is.na(low_volume_flag) | low_volume_flag == 0) %>% 
  filter(is.na(flow_rate_flag) | flow_rate_flag == 0) %>% 
  filter(is.na(is_blank) | is_blank == 0) %>% 
  filter(is.na(negative_pm_mass) | negative_pm_mass == 0) %>% 
  filter(is.na(potential_contamination) | potential_contamination == 0) %>% 
  filter(is.na(bc_mass_ug_corrected) | bc_mass_ug_corrected >= 0.49) %>% 
  filter(campaign %in% paste0("Campaign", c(1, 2, 3, "X"))) %>% 
  rename("pm_ug_m3_raw" = "pm_ug_m3") %>%
  rename("pm_ug_m3" = "pm_ug_m3_dem") %>%
  rename("bc_ug_m3_raw" = "bc_ug_m3") %>%
  rename("bc_ug_m3" = "bc_ug_m3_dem")

highways <- read_csv(here::here("Data", "Highways_AEA.csv")) %>%
  st_as_sf(wkt = "WKT", crs = albers)
```

``` {r site_map, include = T}
ggplot() +
  geom_sf(data = lur_data_sf, aes(color = "pt"), show.legend = "point") +
  geom_sf(data = highways, aes(color = "high"), show.legend = "line") +
  scale_color_manual(name = "Feature",
                     values = c("high" = "red", "pt" = "black"),
                     labels = c("high" = "Highways", "pt" = "Sites")) +
  simple_theme
```

## Temporal trends in the data
Some observations of the data:

- There is a large amount of variability in BC concentrations measured in the winter campaign (Campaign4)
- Concentrations measured at the distributed sites seem to be increasing over time, with a (possible?) downturn at the end of Campaign 4
- This trend isn't reflected in the central site data
- There is an increase in central site BC between 

```{r, echo = F, include = F}
#' Weekly data for each distributed sampling location
week_data <- select(lur_data, StartDateLocal, EndDateLocal, site_id, bc_ug_m3,
                    area_pm, area_no2, area_temp, nn_bc) %>%
  filter(!is.na(StartDateLocal)) %>%
  filter(!is.na(bc_ug_m3)) %>% 
  mutate(plot_date = StartDateLocal + floor((EndDateLocal - StartDateLocal)/2))

week_data$central_bc_check <- NA
week_data$central_pm_check <- NA
week_data$central_temp_check <- NA
week_data$central_no2_check <- NA

#' Calculate average BC, PM2.5, NO2, and temp across all Denver metro monitors
for(i in 1:nrow(week_data)) {
  df <- slice(week_data, i)
  date_list <- seq.Date(df$StartDateLocal, df$EndDateLocal, by = "day")
  
  bc_temp <- filter(bc_data, Date_Local %in% date_list)
  week_data$central_bc_check[i] <- mean(bc_temp$Arithmetic_Mean)
  
  pm_temp <- filter(pm_data, Date_Local %in% date_list)
  week_data$central_pm_check[i] <- mean(pm_temp$Arithmetic_Mean)
  
  no2_temp <- filter(no2_data, Date_Local %in% date_list)
  week_data$central_no2_check[i] <- mean(no2_temp$Arithmetic_Mean)
  
  temp_temp <- filter(temp_data, Date_Local %in% date_list)
  week_data$central_temp_check[i] <- mean(temp_temp$Arithmetic_Mean)
}


cor(week_data$nn_bc, week_data$central_bc_check, use = "complete.obs")
cor(week_data$area_pm, week_data$central_pm_check, use = "complete.obs")
cor(week_data$area_temp, week_data$central_temp_check, use = "complete.obs")
cor(week_data$area_no2, week_data$central_no2_check, use = "complete.obs")

#' Weekly data across all distributed sampling location
week_mean_data <- select(week_data, StartDateLocal, EndDateLocal, site_id, bc_ug_m3,
                         central_pm_check, central_no2_check,
                         central_temp_check, central_bc_check) %>%
  filter(!is.na(StartDateLocal)) %>%
  filter(!is.na(bc_ug_m3))  %>%
  group_by(StartDateLocal, EndDateLocal) %>%
  summarize(bc_ug_m3 = mean(bc_ug_m3, na.rm = T),
            central_pm_check = mean(central_pm_check, na.rm=T),
            central_no2_check = mean(central_no2_check, na.rm=T),
            central_temp_check = mean(central_temp_check, na.rm=T),
            central_bc_check = mean(central_bc_check, na.rm=T)) %>% 
  mutate(plot_date = StartDateLocal + floor((EndDateLocal - StartDateLocal)/2))

#' Study-wide data for each distributed sampling location
site_mean_data <- select(week_data, StartDateLocal, EndDateLocal, site_id, bc_ug_m3,
                         central_pm_check, central_no2_check,
                         central_temp_check, central_bc_check) %>%
  filter(!is.na(StartDateLocal)) %>%
  filter(!is.na(bc_ug_m3))  %>%
  group_by(site_id) %>%
  summarize(bc_ug_m3 = mean(bc_ug_m3, na.rm = T),
            central_pm_check = mean(central_pm_check, na.rm=T),
            central_no2_check = mean(central_no2_check, na.rm=T),
            central_temp_check = mean(central_temp_check, na.rm=T),
            central_bc_check = mean(central_bc_check, na.rm=T))
```

###Time-weighted average BC concentrations at each distributed sampling site
```{r, include = T, echo = F, warning=F}
ggplot() +
  geom_point(data = week_data, aes(x = plot_date, y = bc_ug_m3),
             alpha = 0.25) +
  geom_smooth(data = week_data, aes(x = plot_date, y = bc_ug_m3)) +
  ylab("Distributed site BC for individual filters") + xlab("Sampling date") +
  simple_theme
```

###"Weekly" mean BC across all sampling sites (averaged based on the start and end dates of each sampling period)
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_mean_data, aes(x = plot_date, y = bc_ug_m3),
             size = 1) +
  geom_smooth(data = week_mean_data, aes(x = plot_date, y = bc_ug_m3)) +
  ylab("Weekly average distributed Site BC") + xlab("Sampling date") +
  simple_theme
```

###Distribution of BC by sampling campaign and week
```{r include = T, echo = F}
box_data <- lur_data %>% 
  select(site_id, campaign, week, bc_ug_m3) %>% 
  mutate(week_id = paste(campaign, week, sep = "-"))

ggplot(box_data) +
  geom_boxplot(aes(x = week_id, y = bc_ug_m3, color = as.factor(campaign))) +
  scale_color_viridis(name = "Campaign", discrete = T) +
  xlab("Campaign week") + ylab("Distributed site BC concentration") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  simple_theme

```

###Temporal trends in central site monitor BC, central site PM, and central site temperature
```{r, include = T, echo = F}

ggplot(bc_data) +
  geom_point(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  geom_line(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  geom_smooth(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  scale_color_viridis(discrete = T, name = "Monitor ID") +
  xlab("Date") + ylab("Central site BC") +
  simple_theme

ggplot(pm_data) +
  geom_point(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id),
             size = 0.5) +
  geom_line(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  geom_smooth(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  scale_color_viridis(discrete = T, name = "Monitor ID") +
  xlab("Date") + ylab("Central site PM") +
  facet_wrap(. ~ monitor_id) +
  simple_theme

ggplot(no2_data) +
  geom_point(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id),
             size = 0.5) +
  geom_line(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  geom_smooth(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  scale_color_viridis(discrete = T, name = "Monitor ID") +
  xlab("Date") + ylab("Central site NO2") +
  facet_wrap(. ~ monitor_id) +
  simple_theme

ggplot(temp_data) +
  geom_point(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id),
             size = 0.5) +
  geom_line(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  geom_smooth(aes(x = Date_Local, y = Arithmetic_Mean, color = monitor_id)) +
  scale_color_viridis(discrete = T, name = "Monitor ID") +
  xlab("Date") + ylab("Central site temperature") +
  facet_wrap(. ~ monitor_id) +
  simple_theme

```

###Comparing the central site BC variability (daily) to the distributed site BC variability (weekly)
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = plot_date, y = bc_ug_m3, color = "dist")) +
  geom_smooth(data = week_data, aes(x = plot_date, y = bc_ug_m3, color = "dist")) +
  geom_point(data = bc_data, aes(x = Date_Local, y = Arithmetic_Mean, color = "cent")) +
  geom_smooth(data = bc_data, aes(x = Date_Local, y = Arithmetic_Mean, color = "cent")) +
  scale_color_viridis(discrete = T, name = "Sampling location",
                      labels = c("dist" = "Distributed sites",
                                 "cent" = "Central monitoring site")) +
  ylab("Weekly BC concentrations ") + xlab("Sampling Start Date") +
  simple_theme
```

###Comparing the central site BC variability (averaged to the sampling period of each distributed site sample) to the distributed site BC variability
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = plot_date, y = bc_ug_m3, color = "dist")) +
  geom_smooth(data = week_data, aes(x = plot_date, y = bc_ug_m3, color = "dist")) +
  geom_point(data = week_data, aes(x = plot_date, y = central_bc_check, color = "cent")) +
  geom_smooth(data = week_data, aes(x = plot_date, y = central_bc_check, color = "cent")) +
  scale_color_viridis(discrete = T, name = "Sampling location",
                      labels = c("dist" = "Distributed sites",
                                 "cent" = "Central monitoring site")) +
  ylab("Weekly BC concentrations ") + xlab("Sampling date") +
  simple_theme
```

## Relationships between distributed site and central site BC

###Relationships between the distributed site BC and the central monitor are weak. The correlation coefficient for distributed site BC (time weighted average) to central site BC (averaged to the same sampling period for each filter) is `r cor(week_data$bc_ug_m3, week_data$central_bc_check)`
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = bc_ug_m3, y = central_bc_check),
             alpha = 0.25) +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_bc_check,
                                    color = "lin"), method = "lm") +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_bc_check,
                                    color = "loess")) +
  scale_color_viridis(discrete = T, name = "Smoothing method",
                      labels = c("lin" = "Linear model", "loess" = "LOESS")) +
  xlab("TWA BC from distributed site") + 
  ylab("Central monitoring site BC\naveraged to match distributed site periods") +
  simple_theme
```

###The relationship looks different when we average all distributed sites (based on sampling start and end date). Here the data are averaged across all sites with the same start and end sampling date 
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_mean_data, aes(x = bc_ug_m3, y = central_bc_check)) +
  geom_smooth(data = week_mean_data, aes(x = bc_ug_m3, y = central_bc_check,
                                    color = "lin"), method = "lm") +
  geom_smooth(data = week_mean_data, aes(x = bc_ug_m3, y = central_bc_check,
                                    color = "loess")) +
  scale_color_viridis(discrete = T, name = "Smoothing method",
                      labels = c("lin" = "Linear model", "loess" = "LOESS")) +
  xlab("Average across all distributed sites with the same start and end dates") + 
  ylab("Central monitoring site BC\naveraged to match distributed site periods") +
  simple_theme
```

###Relationships between central site PM~2.5~ (averaged across all monitors in the area) and distributed site BC. Here PM~2.5~ is averaged for the same sampling start and end dates as the distributed site measurements. The correlation coefficient for distributed site BC (time weighted average) to central site PM~2.5~ (averaged to the same sampling period for each filter) is `r cor(week_data$bc_ug_m3, week_data$central_pm_check, use = "complete.obs")`
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = bc_ug_m3, y = central_pm_check),
             alpha = 0.25) +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_pm_check,
                                    color = "lin"), method = "lm") +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_pm_check,
                                    color = "loess")) +
  scale_color_viridis(discrete = T, name = "Smoothing method",
                      labels = c("lin" = "Linear model", "loess" = "LOESS")) +
  xlab("TWA BC from distributed site") + 
  ylab("Central monitoring site PM2.5\naveraged to match distributed site periods") +
  simple_theme
```

###Relationships between central site temperature (averaged across all monitors in the area) and distributed site BC. Here temperature is averaged for the same sampling start and end dates as the distributed site measurements. The correlation coefficient for distributed site BC (time weighted average) to central site temperature (averaged to the same sampling period for each filter) is `r cor(week_data$bc_ug_m3, week_data$central_temp_check, use = "complete.obs")`
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = bc_ug_m3, y = central_temp_check),
             alpha = 0.25) +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_temp_check,
                                    color = "lin"), method = "lm") +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_temp_check,
                                    color = "loess")) +
  scale_color_viridis(discrete = T, name = "Smoothing method",
                      labels = c("lin" = "Linear model", "loess" = "LOESS")) +
  xlab("TWA BC from distributed site") + 
  ylab("Central monitoring site temperature\naveraged to match distributed site periods") +
  simple_theme
```

###Relationships between central site NO2 (averaged across all monitors in the area) and distributed site BC. Here NO2 is averaged for the same sampling start and end dates as the distributed site measurements. The correlation coefficient for distributed site BC (time weighted average) to central site temperature (averaged to the same sampling period for each filter) is `r cor(week_data$bc_ug_m3, week_data$central_no2_check, use = "complete.obs")`.
```{r, include = T, echo = F}
ggplot() +
  geom_point(data = week_data, aes(x = bc_ug_m3, y = central_no2_check),
             alpha = 0.25) +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_no2_check,
                                    color = "lin"), method = "lm") +
  geom_smooth(data = week_data, aes(x = bc_ug_m3, y = central_no2_check,
                                    color = "loess")) +
  scale_color_viridis(discrete = T, name = "Smoothing method",
                      labels = c("lin" = "Linear model", "loess" = "LOESS")) +
  xlab("TWA BC from distributed site") + 
  ylab("Central monitoring site NO2\naveraged to match distributed site periods") +
  simple_theme
```

## Charaterizing spatial and temporal variability
###What is the distribution of coefficient of variation (CV) values for each sampling location?

Across all distributed site filters, the CV for black carbon measurements is `r round(sd(lur_data$bc_ug_m3, na.rm = T)/mean(lur_data$bc_ug_m3, na.rm = T), 2)*100`%
``` {r, echo = F, include = F}
#' Study-wide data for each sampling location
site_summary <- select(lur_data, StartDateLocal, site_id, bc_ug_m3,
                       nn_pm, nn_no2, nn_temp, nn_bc,
                       idw_pm, idw_no2, idw_temp,
                       area_pm, area_no2, area_temp) %>%
  filter(!is.na(StartDateLocal)) %>%
  filter(!is.na(bc_ug_m3))  %>%
  group_by(site_id) %>%
  summarize(mean_bc = mean(bc_ug_m3, na.rm = T),
            sd_bc = sd(bc_ug_m3, na.rm = T),
            cv_bc = sd(bc_ug_m3, na.rm = T)/mean(bc_ug_m3, na.rm = T))

#' Overall CV
sd(site_summary$mean_bc, na.rm = T)/mean(site_summary$mean_bc, na.rm = T)
sd(lur_data$bc_ug_m3, na.rm = T)/mean(lur_data$bc_ug_m3, na.rm = T)

```

Some distributed sampling locations have a large amount of variability (CV > 0.25) in weekly BC concentrations measured across the study period compared to others. 
```{r , include = T, echo = F}
ggplot(site_summary) +
  geom_histogram(aes(x = cv_bc)) +
  xlab("CV of BC measurements at each sampling location") +
  simple_theme
```

# Data Decisions: Transformation? Include Campaign 4?
We need to decide if we want to use a log-transformation. We also need to decide if we want to include Campaign 4. As detailed above, there is something wonky about those data. We have a second "winter" campaign running now, but the UPAS aren't co-located with the aethalometer, so it's unclear if we can use those data to check to see if our original winter campaign was successful or not.

Update! John and Ben's theory is that lower temps messed with our runtimes. We checked in with Scott Weichenthal (McGill) since he is also trying to use the UPAS as outdoor monitors. He noted that at cold temps the monitors just shut down, and that they needed to repeat a campaign because of that, so perhaps there's some validity to that idea. The PM data may have been less affected given higher PM concentrations in the winter in general.

```{r, include = F, echo = F}

#' Filter out unusable filters
#' Drop filters where the BC mass is below 0.49 ug (Quinn et al., 2018)  
data_name <- "Combined_Filter_Data_AEA.csv"
lur_data <- read_csv(here::here("Data", data_name)) %>% 
  filter(!is.na(lon)) %>% 
  filter(indoor == 0) %>% 
  filter(bc_ug_m3_dem > 0) %>% 
  filter(is.na(below_lod) | below_lod == 0) %>% 
  filter(is.na(low_volume_flag) | low_volume_flag == 0) %>% 
  filter(is.na(flow_rate_flag) | flow_rate_flag == 0) %>% 
  filter(is.na(is_blank) | is_blank == 0) %>% 
  filter(is.na(negative_pm_mass) | negative_pm_mass == 0) %>% 
  filter(is.na(potential_contamination) | potential_contamination == 0) %>% 
  filter(is.na(bc_mass_ug_corrected) | bc_mass_ug_corrected >= 0.49) %>% 
  filter(campaign %in% paste0("Campaign", c(1, 2, 3, "X")))

lur_data <- lur_data %>%
  rename("pm_ug_m3_raw" = "pm_ug_m3") %>%
  rename("pm_ug_m3" = "pm_ug_m3_dem") %>%
  rename("bc_ug_m3_raw" = "bc_ug_m3") %>%
  rename("bc_ug_m3" = "bc_ug_m3_dem")

#' Format the date variable to be Year-WeekNo. (starts on Sunday)
#' Convert that back to the first day of the week
lur_data <- lur_data %>%
  mutate(sample_week_no = format(StartDateLocal, "%Y-%W")) %>%
  mutate(sample_week = as.Date(cut(as.Date(StartDateLocal), "week")))

#' Add a unique site ID
lur_data <- mutate(lur_data, site_id_lonlat = paste(lon, lat, sep = "_"))

ids <- select(lur_data, site_id_lonlat) %>%
  distinct() %>%
  mutate(site_id = paste("d", seq_along(site_id_lonlat), sep = "_"))

write_csv(ids, here::here("Data", "ST_Model_Site_IDs.csv"))

lur_data <- left_join(lur_data, ids, by = "site_id_lonlat") %>%
  mutate(site_id = ifelse(filter_id == "080310027", "central", site_id)) %>%
  arrange(sample_week)

lur_years <- seq(year(lur_data$sample_week[1]),
                 year(lur_data$sample_week[nrow(lur_data)]),
                 by = 1)
lur_years
```

## Comparing distributions for BC and log-transformed BC

### BC from all campaigns with and without log-transformation
```{r, echo = F}

par(mar = c(4, 4, 0.1, 0.1))
histogram(lur_data$bc_ug_m3, main = "BC in original units")
histogram(log(lur_data$bc_ug_m3), main = "log-tranformed BC")
```

### BC after dropping Campaign 4 (winter) with and without log-transformation

```{r, echo = F}

lur_data2 <- filter(lur_data, campaign %in% c(paste0("Campaign", c(1, 2, 3, "X"))))

par(mar = c(4, 4, 0.1, 0.1))
histogram(lur_data2$bc_ug_m3, main = "BC in original units")
histogram(log(lur_data2$bc_ug_m3), main = "log-tranformed BC")
```

At this point, I'm going to drop Campaign 4 and move forward with trying to fit the model using the reduced data set. I'll be using the log-transformed BC concentrations.

# Formatting the data for the ```Spatiotemporal``` package 

<!-- ```{r, echo = T, include = F} -->
<!-- data(mesa.data.raw, package="SpatioTemporal") -->
<!-- names(mesa.data.raw) -->

<!-- head(mesa.data.raw$X) # Spatial covariates -->
<!-- class(mesa.data.raw$X) -->

<!-- head(mesa.data.raw$obs) # Observations at each sampling site (wide: time vs. location) -->
<!-- class(mesa.data.raw$obs) -->

<!-- head(mesa.data.raw$lax.conc.1500) # ST covariates-- in this case, the CALINE output -->
<!-- class(mesa.data.raw$lax.conc.1500) # Separate matrices for each predictor -->
<!-- ``` -->

I need to get data into the correct format to be able to run the model. Here I am using the log-transformed concentrations and dropping the Campaign 4 data.

## Issue with Site 61
For initial iterations of this model, Site 61 looked much different from the others, so I dropped it for now. I'll need to go back and figure out what was wonky about that site later.

Update: Dropping Campaign 4 removed site 61, so no need to dig in unless we want to add those results back.

<!-- Something is going on with Site 61. When calculating the beta fields, I got results that were an order of magnitude larger than for the other sites. How does this site differ from the others? -->

<!-- The plots below suggest that maybe there is a different temporal pattern at this site? It's not far from other sites, but we only have one campaign's worth of data here. I'm going to drop it for now, but I'll plan to come back to this eventually. -->
<!-- ```{r} -->
<!-- d61_data <- filter(lur_data2, site_id == "d_61") %>% -->
<!--   st_as_sf(wkt = "WKT", crs = albers) -->
<!-- other_data <- filter(lur_data2, site_id != "d_61") %>% -->
<!--   st_as_sf(wkt = "WKT", crs = albers) -->

<!-- plot(st_geometry(other_data), col = "red", pch = 16) -->
<!-- plot(st_geometry(d61_data), col = "blue", add = T, pch = 17) -->
<!-- legend(x = "topleft", c("Other sites", "Site 61"), -->
<!--        pch = c(16, 17), -->
<!--        col = c("red", "blue")) -->

<!-- ggplot() + -->
<!--   geom_boxplot(data = d61_data, aes(x = 1, y = bc_ug_m3, col = "d61")) + -->
<!--   geom_boxplot(data = other_data, aes(x = 2, y = bc_ug_m3, col = "all")) + -->
<!--   scale_color_viridis(name = "Site", discrete = T, -->
<!--                       labels = c("d61" = "Site 61", -->
<!--                                  "all" = "All other sites")) -->

<!-- ggplot() + -->
<!--   geom_line(data = d61_data, aes(x = sample_week, y = bc_ug_m3, col = "d61")) + -->
<!--   geom_line(data = filter(other_data, filter_id == "080310027"), -->
<!--             aes(x = sample_week, y = bc_ug_m3, col = "cent")) + -->
<!--   scale_color_viridis(name = "Site", discrete = T, -->
<!--                       labels = c("d61" = "Site 61", -->
<!--                                  "cent" = "Central BC")) -->

<!-- #' Filter out site 61 -->
<!-- lur_data2 <- filter(lur_data2, site_id != "d_61") -->
<!-- ``` -->

## Observations of black carbon at the distributed and central sites.
Going to set up objects with log-transformed data and non-transformed BC data.

```{r}
# With log transformation
bc_obs <- lur_data2 %>%
  select(site_id, sample_week, bc_ug_m3) %>%
  mutate(log_bc = log(bc_ug_m3)) %>%
  select(-bc_ug_m3) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = log_bc) %>%
              # names_from = site_id, values_from = bc_ug_m3) %>%
  as.data.frame() %>%
  arrange(sample_week)
rownames(bc_obs) <- bc_obs$sample_week
bc_obs$sample_week <- NULL
bc_obs <- as.matrix(bc_obs)

bc_weeks <- rownames(bc_obs)

# rownames(bc_obs)
# colnames(bc_obs)
class(bc_obs)
dim(bc_obs)

# # Without log-transformation
# bc_obs.nt <- lur_data2 %>%
#   select(site_id, sample_week, bc_ug_m3) %>%
#   # mutate(log_bc = log(bc_ug_m3)) %>%
#   # select(-bc_ug_m3) %>%
#   pivot_wider(id_cols = sample_week,
#               # names_from = site_id, values_from = log_bc) %>%
#               names_from = site_id, values_from = bc_ug_m3) %>%
#   as.data.frame() %>%
#   arrange(sample_week)
# rownames(bc_obs.nt) <- bc_obs.nt$sample_week
# bc_obs.nt$sample_week <- NULL
# bc_obs.nt <- as.matrix(bc_obs.nt)
# 
# bc_weeks.nt <- rownames(bc_obs.nt)
# 
# # rownames(bc_obs)
# # colnames(bc_obs)
# class(bc_obs.nt)
# dim(bc_obs.nt)
```

## Spatial covariates at the distributed site 
These are based on the LASSO model (THIS WILL CHANGE IN THE FUTURE!! I just know how to use LASSO vs. PLS). Make sure the covariates are scaled here. Here I'm using the variables selected by the LASSO model using only three campaigns worth of data and a log-transformed outcome.

```{r, include = F}
#' Log-transformed data
# load(here::here("Results", "BC_LASSO_Model.Rdata"))
load(here::here("Results", "BC_LASSO_Model_3C.Rdata"))

lasso_coef_df <- data.frame(name = log_bc_lasso_coef3@Dimnames[[1]][log_bc_lasso_coef3@i + 1],
                            coefficient = log_bc_lasso_coef3@x)
# lasso_coef_df
covars <- as.character(lasso_coef_df$name[-c(1:2)])
covars 

covar_fun <- paste("~", paste(covars, collapse = " + "))
covar_fun
```

```{r}

#' log-tranformed data
covars
bc_sp_cov <- select(lur_data2, site_id, lon, lat, all_of(covars)) %>%
  distinct() %>%
  mutate_at(.vars = vars(covars),
            scale)

bc_sp_cov <- bc_sp_cov %>%
  rename(ID = site_id) %>%
  mutate(lon2 = lon, lat2 = lat) %>%
  st_as_sf(coords = c("lon2", "lat2"), crs = ll_wgs84) %>%
  st_transform(crs = albers)
sp_coords <- do.call(rbind, st_geometry(bc_sp_cov)) %>%
  as_tibble() %>% setNames(c("x","y"))
bc_sp_cov <- bind_cols(bc_sp_cov, sp_coords) %>%
  st_set_geometry(NULL) %>%
  as.data.frame()

bc_sp_cov$type <- ifelse(bc_sp_cov$ID == "central", "central", "dist")
bc_sp_cov$type <- as.factor(bc_sp_cov$type)
# bc_sp_cov$type

# head(bc_sp_cov)
# class(bc_sp_cov)
cor(bc_sp_cov[,covars])

#' Untransformed data
# covars.nt
# bc_sp_cov.nt <- select(lur_data2, site_id, lon, lat, covars.nt) %>%
#   distinct() %>%
#   mutate_at(.vars = vars(covars.nt),
#             scale)
# 
# bc_sp_cov.nt <- bc_sp_cov.nt %>%
#   rename(ID = site_id) %>%
#   mutate(lon2 = lon, lat2 = lat) %>%
#   st_as_sf(coords = c("lon2", "lat2"), crs = ll_wgs84) %>%
#   st_transform(crs = albers)
# sp_coords.nt <- do.call(rbind, st_geometry(bc_sp_cov.nt)) %>%
#   as_tibble() %>% setNames(c("x","y"))
# bc_sp_cov.nt <- bind_cols(bc_sp_cov.nt, sp_coords.nt) %>%
#   st_set_geometry(NULL) %>%
#   as.data.frame()
# 
# bc_sp_cov.nt$type <- ifelse(bc_sp_cov.nt$ID == "central", "central", "dist")
# bc_sp_cov.nt$type <- as.factor(bc_sp_cov.nt$type)
# 
# # head(bc_sp_cov.nt)
# # class(bc_sp_cov.nt)
# cor(bc_sp_cov.nt[,c(4:17)])
```

## Spatiotemporal covariates at the distributed sites
We have available IDW estimates of temp, NO2, PM2.5, and BC. For now, just using NO2, since the correlations with distributed site log(BC) are decent. I originally tried to use two ST predictors, but ran into issues trying to fit the model.

```{r}
#' log-transformed data
cor(log(lur_data2$bc_ug_m3), lur_data2$idw_no2, use = "complete")
cor(log(lur_data2$bc_ug_m3), lur_data2$idw_pm, use = "complete")
cor(log(lur_data2$bc_ug_m3), lur_data2$idw_temp, use = "complete")

#' Original units
# cor(lur_data2$bc_ug_m3, lur_data2$idw_no2, use = "complete")
# cor(lur_data2$bc_ug_m3, lur_data2$idw_pm, use = "complete")
# cor(lur_data2$bc_ug_m3, lur_data2$idw_temp, use = "complete")
```

```{r, include = F, echo = F}
bc_st_pm <- select(lur_data2, site_id, sample_week, idw_pm) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = idw_pm) %>%
  as.data.frame()
rownames(bc_st_pm) <- bc_st_pm$sample_week
bc_st_pm$sample_week <- NULL
bc_st_pm <- as.matrix(bc_st_pm)

bc_st_no2 <- select(lur_data2, site_id, sample_week, idw_no2) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = idw_no2) %>%
  as.data.frame()
rownames(bc_st_no2) <- bc_st_no2$sample_week
bc_st_no2$sample_week <- NULL
bc_st_no2 <- as.matrix(bc_st_no2)

bc_st_temp <- select(lur_data2, site_id, sample_week, idw_temp) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = idw_temp) %>%
  as.data.frame()
rownames(bc_st_temp) <- bc_st_temp$sample_week
bc_st_temp$sample_week <- NULL
bc_st_temp <- as.matrix(bc_st_temp)
```

## Create the ST data object and determine the number of basis functions for the temporal trend
For now, going to use one ST predictor (NO2 estimated for each sampling location).
Josh advised 4 degrees of freedom per year. 

Based on the plots below, it looks like we can still reasonably use one basis function for now, but I might try two in the future since that's where the metrics seem to level off. There might be some advantage to using more than 4 df per year. I'll need to ask Josh if these results suggest we need to add df to the basis functions (over the 4/year he originally suggested as a starting point).

```{r}
# With log-transformation
denver.data <- createSTdata(obs = bc_obs,
                            covars = bc_sp_cov,
                            SpatioTemporal = list(bc_st_no2 = bc_st_no2))
D <- createDataMatrix(denver.data)
# names(denver.data)
# print(denver.data)

# Determine the number of basis functions
n_years <- length(unique(as.Date(cut(as.Date(rownames(bc_obs)), "year"))))

SVD.cv.4py <- SVDsmoothCV(D, 0:4, df = 4*n_years)
print(SVD.cv.4py)
plot(SVD.cv.4py)

n_df <- 4
n_basis <- 1

# Add the smooth temporal basis functions to the ST data object
denver.data <- updateTrend(denver.data, n.basis = n_basis, df = n_df*n_years)

head(denver.data$trend)
plot(denver.data$trend$date, denver.data$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "BC",
     main = "Basis function (with log-transformation)")
lines(denver.data$trend$date, denver.data$trend$V1, col = 2)

layout(matrix(c(1,2,1,3), 2, 2))
par(mar=c(2.3,3.3,2,1), mgp=c(2,1,0))
plot(denver.data, "loc", main="Occurrence of Observations", xlab="",
     ylab="Location", col=c("black", "red"), legend.loc=NULL)
par(mar=c(3.3,3.3,2,1))
qqnorm(denver.data, line=1)
scatterPlot(denver.data, covar="aadt_100", xlab="AADT in a 100 m buffer",
            ylab="BC (log ug/m3)", pch=19, cex=.25,
            smooth.args=list(span=4/5,degree=2))
```

## Evaluate the basis functions
Plotting the temporal trends at a few of the sites (using the log-transformed data)
```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data, "acf", ID="d_1")
plot(denver.data, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data, "acf", ID="d_20")
plot(denver.data, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data, "acf", ID="d_53")
plot(denver.data, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data, "acf", ID="central")
plot(denver.data, "pacf", ID="central")
```

# Initial models (BC for temporal trends)
We first fit models using only the BC measurements taken at the aethalometer to determine temporal trends 

## Model 1.1: simplest form 

### Create the model object 
For this version of the model, use ```iid``` for both ```cov.beta``` (beta0 and beta1) and ```cov.nu``` (error term).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data$covars)

LUR1.1 <- list(covar_fun, covar_fun)

cov.beta1.1 <-  list(covf="iid", nugget = T)
cov.nu1.1 <- list(covf="iid", nugget = T, random.effect = FALSE)
locations1.1 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.1.1 <- createSTmodel(denver.data, LUR = LUR1.1,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta1.1, cov.nu = cov.nu1.1,
                                  locations = locations1.1)
denver.model.1.1
```

### Estimate model parameters

We want to try a number of initial conditions to make sure we find the "right" solution.

```{r}
names <- loglikeSTnames(denver.model.1.1, all=FALSE)
names

# x.init.1.1 <- cbind(c(rep(-1, 3)), c(rep(-2, 3)), c(rep(-3, 3)),
#                     c(rep(-4, 3)), c(rep(-5, 3)), c(rep(-6, 3)))
# x.init.1.1[nrow(x.init.1.1),] <- 0

x.init.1.1 <- cbind(c(0, 0, 0),
                    c(-5, -5, -5),
                    c(-8, -8, -5),
                    c(-8, -8, -8))

rownames(x.init.1.1) <- loglikeSTnames(denver.model.1.1, all=FALSE)
x.init.1.1

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.1.1 <- estimate.STmodel(denver.model.1.1, x.init.1.1)
print(est.denver.model.1.1)
```

### Cross-validation

Define the CV groups (and don't forget to set the seed here!)
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.1.1 <- createCV(denver.model.1.1, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.1.1 <- sapply(split(denver.model.1.1$obs$ID, Ind.cv.1.1), unique)
print(sapply(ID.cv.1.1, length))
table(Ind.cv.1.1)

I.col.1.1 <- apply(sapply(ID.cv.1.1,function(x) denver.model.1.1$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.1.1) <- denver.model.1.1$locations$ID
print(I.col.1.1)

par(mfrow=c(1,1))
plot(denver.model.1.1$locations$long,
     denver.model.1.1$locations$lat,
     pch=23+floor(I.col.1.1/max(I.col.1.1)+.5), bg=I.col.1.1,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.1.1.cv <- coef(est.denver.model.1.1, pars="cov")[,c("par","init")]
x.init.1.1.cv
```

Run the model with cross validation.
```{r}
est.denver.1.1.cv <- estimateCV(denver.model.1.1, x.init.1.1.cv, Ind.cv.1.1)
print(est.denver.1.1.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.1.1, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.1.1.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data, denver.model.1.1, est.denver.model.1.1, est.denver.1.1.cv,
     file = here::here("Results", "Denver_ST_Model_1.1.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.1.1.cv <- predictCV(denver.model.1.1, est.denver.1.1.cv, LTA = T)
pred.1.1.cv.log <- predictCV(denver.model.1.1, est.denver.1.1.cv,
                             LTA = T, transform="unbiased")

names(pred.1.1.cv)
summary(pred.1.1.cv)
summary(pred.1.1.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod1.1.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 1.2: Smoothing in the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta0 and beta1) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data$covars)

LUR1.2 <- list(covar_fun, covar_fun)

cov.beta1.2 <-  list(covf = c("iid", "iid"), nugget = T)
cov.nu1.2 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations1.2 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.1.2 <- createSTmodel(denver.data, LUR = LUR1.2,
                                      ST = "bc_st_no2",
                                      cov.beta = cov.beta1.2, cov.nu = cov.nu1.2,
                                      locations = locations1.2)
denver.model.1.2
```

### Estimate model parameters

Josh gave some guidance on how to set up the initial values (NOTE: the ```iid``` model doesn't have range or sill values, but the ```exp``` model does):

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.1.2, all=FALSE)
names

# x.init.1.2 <- cbind(c(0, 0, 0, 0, 0),
#                         c(-1, -1, 0, -1, -1),
#                         c(-1, -1, 0, -5, -1),
#                         c(-5, -5, 0, -1, -5),
#                         c(-5, -5, 0, -5, -5),
#                         c(-1, -1, 2, -1, -1),
#                         c(-1, -1, 2, -5, -1),
#                         c(-5, -5, 2, -1, -5),
#                         c(-5, -5, 2, -5, -5),
#                         c(-1, -1, 4, -1, -1),
#                         c(-1, -1, 4, -5, -1),
#                         c(-5, -5, 4, -1, -5),
#                         c(-5, -5, 4, -5, -5))

x.init.1.2 <- cbind(c(0, 0, 0, 0, 0),
                        c(-5, -5, 4, -3, -5),
                        c(-5, -5, 4, -5, -5),
                        c(-5, -5, 6, -3, -5),
                        c(-5, -5, 6, -5, -5),
                        c(-5, -5, 8, -3, -5),
                        c(-5, -5, 8, -5, -5))

rownames(x.init.1.2) <- loglikeSTnames(denver.model.1.2, all=FALSE)
x.init.1.2

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.1.2 <- estimate.STmodel(denver.model.1.2, x.init.1.2)
print(est.denver.model.1.2)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.1.2 <- createCV(denver.model.1.2, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.1.2 <- sapply(split(denver.model.1.2$obs$ID, Ind.cv.1.2), unique)
print(sapply(ID.cv.1.2, length))
table(Ind.cv.1.2)

I.col.1.2 <- apply(sapply(ID.cv.1.2, function(x) denver.model.1.2$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.1.2) <- denver.model.1.2$locations$ID
print(I.col.1.2)

par(mfrow=c(1,1))
plot(denver.model.1.2$locations$long,
     denver.model.1.2$locations$lat,
     pch=23+floor(I.col.1.2/max(I.col.1.2)+.5), bg=I.col.1.2,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.1.2.cv <- coef(est.denver.model.1.2, pars="cov")[,c("par","init")]
x.init.1.2.cv
```

Run the model with cross validation
```{r}

est.denver.1.2.cv <- estimateCV(denver.model.1.2, x.init.1.2.cv, Ind.cv.1.2)
print(est.denver.1.2.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.1.2, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.1.2.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data, denver.model.1.2, est.denver.model.1.2, est.denver.1.2.cv,
     file = here::here("Results", "Denver_ST_Model_1.2.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.1.2.cv <- predictCV(denver.model.1.2, est.denver.1.2.cv, LTA = T)
pred.1.2.cv.log <- predictCV(denver.model.1.2, est.denver.1.2.cv,
                             LTA = T, transform="unbiased")

names(pred.1.2.cv)
summary(pred.1.2.cv)
summary(pred.1.2.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod1.2.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```


## Model 1.3: Smoothing in beta0 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta1) and ```exp``` for ```cov.beta``` (beta0) ```cov.nu``` (error).

Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data$covars)

LUR1.3 <- list(covar_fun, covar_fun)

cov.beta1.3 <-  list(covf = c("exp", "iid"), nugget = T)
cov.nu1.3 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations1.3 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.1.3 <- createSTmodel(denver.data, LUR = LUR1.3,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta1.3, cov.nu = cov.nu1.3,
                                  locations = locations1.3)
denver.model.1.3
```

### Estimate model parameters

Josh gave some guidance on how to set up the initial values (NOTE: the ```iid``` model doesn't have range or sill values, but the ```exp``` model does):

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.1.3, all=FALSE)
names

# x.init.1.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, -5, 0, -1, -1),
#                     c(0, -5, -1, -1, 0, -5, -5),
#                     c(0, -5, -5, -5, 0, -5, -5),
#                     c(2, -1, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, -5, 2, -1, -1),
#                     c(2, -5, -1, -1, 2, -5, -5),
#                     c(2, -5, -5, -5, 2, -5, -5),
#                     c(4, -1, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, -5, 4, -1, -1),
#                     c(4, -5, -1, -1, 4, -5, -5),
#                     c(4, -5, -5, -5, 4, -5, -5),
#                     c(6, -1, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, -5, 6, -1, -1),
#                     c(6, -5, -1, -1, 6, -5, -5),
#                     c(6, -5, -5, -5, 6, -5, -5)
#                     )
# x.init.1.3[nrow(x.init.1.3),] <- 0

x.init.1.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
                    c(6, -1, -1, -1, 6, -1, -1),
                    c(6, -1, -5, -5, 6, -1, -1),
                    c(6, -10, -1, -1, 6, -3, -5),
                    c(6, -10, -5, -5, 6, -3, -5),
                    c(6, -1, -1, -1, 10, -1, -1),
                    c(6, -1, -5, -5, 10, -1, -1),
                    c(6, -10, -1, -1, 10, -3, -5),
                    c(6, -10, -5, -5, 10, -3, -5)
                    )
x.init.1.3[nrow(x.init.1.3),] <- -5

rownames(x.init.1.3) <- loglikeSTnames(denver.model.1.3, all=FALSE)
x.init.1.3

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.1.3 <- estimate.STmodel(denver.model.1.3, x.init.1.3)
print(est.denver.model.1.3)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.1.3 <- createCV(denver.model.1.3, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.1.3 <- sapply(split(denver.model.1.3$obs$ID, Ind.cv.1.3), unique)
print(sapply(ID.cv.1.3, length))
table(Ind.cv.1.3)

I.col.1.3 <- apply(sapply(ID.cv.1.3, function(x) denver.model.1.3$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.1.3) <- denver.model.1.3$locations$ID
print(I.col.1.3)

par(mfrow=c(1,1))
plot(denver.model.1.3$locations$long,
     denver.model.1.3$locations$lat,
     pch=23+floor(I.col.1.3/max(I.col.1.3)+.5), bg=I.col.1.3,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.1.3.cv <- coef(est.denver.model.1.3, pars="cov")[,c("par","init")]
x.init.1.3.cv
```

Run the model with cross validation
```{r}

est.denver.1.3.cv <- estimateCV(denver.model.1.3, x.init.1.3.cv, Ind.cv.1.3)
print(est.denver.1.3.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.1.3, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.1.3.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data, denver.model.1.3, est.denver.model.1.3, est.denver.1.3.cv,
     file = here::here("Results", "Denver_ST_Model_1.3.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.1.3.cv <- predictCV(denver.model.1.3, est.denver.1.3.cv, LTA = T)
pred.1.3.cv.log <- predictCV(denver.model.1.3, est.denver.1.3.cv,
                             LTA = T, transform="unbiased")

names(pred.1.3.cv)
summary(pred.1.3.cv)
summary(pred.1.3.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod1.3.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 1.4: Smoothing for beta1 and error term

For this version of the model, use ```iid``` for ```cov.beta``` (beta0) and ```exp``` for ```cov.beta``` (beta1) ```cov.nu``` (error).

### Create the model object 

```{r}
names(denver.data$covars)

LUR1.4 <- list(covar_fun, covar_fun)

cov.beta1.4 <-  list(covf=c("iid", "exp"), nugget = c(TRUE, TRUE))
cov.nu1.4 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations1.4 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.1.4 <- createSTmodel(denver.data, LUR = LUR1.4,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta1.4, cov.nu = cov.nu1.4,
                                  locations = locations1.4)
denver.model.1.4
```

### Estimate model parameters 

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.1.4, all=FALSE)
names

# x.init.1.4 <- cbind(c(0, 0, 0, 0, 0),
#                     c(-1, 0, -1, -1, 0, -1, -1),
#                     c(-1, 0, -1, -5, 0, -5, -1),
#                     c(-5, 0, -5, -1, 0, -1, -5),
#                     c(-5, 0, -5, -5, 0, -5, -5),
#                     c(-1, 2, -1, -1, 2, -1, -1),
#                     c(-1, 2, -1, -5, 2, -5, -1),
#                     c(-5, 2, -5, -1, 2, -1, -5),
#                     c(-5, 2, -5, -5, 2, -5, -5),
#                     c(-1, 4, -1, -1, 4, -1, -1),
#                     c(-1, 4, -1, -5, 4, -5, -1),
#                     c(-5, 4, -5, -5, 4, -1, -5),
#                     c(-5, 4, -5, -5, 4, -5, -5))
# x.init.1.4[nrow(x.init.1.4),] <- 0

x.init.1.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
                    c(-5, 4, -5, -1, 8, -1, -5),
                    c(-5, 4, -5, -5, 8, -5, -5),
                    c(-1, 4, -1, -1, 8, -1, -5),
                    c(-1, 4, -1, -5, 8, -5, -5),
                    c(-5, 4, -10, -1, 8, -1, -5),
                    c(-5, 4, -10, -5, 8, -5, -5))

rownames(x.init.1.4) <- loglikeSTnames(denver.model.1.4, all=FALSE)
x.init.1.4

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.1.4 <- estimate.STmodel(denver.model.1.4, x.init.1.4)
print(est.denver.model.1.4)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.1.4 <- createCV(denver.model.1.4, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.1.4 <- sapply(split(denver.model.1.4$obs$ID, Ind.cv.1.4), unique)
print(sapply(ID.cv.1.4, length))
table(Ind.cv.1.4)

I.col.1.4 <- apply(sapply(ID.cv.1.4,function(x) denver.model.1.4$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.1.4) <- denver.model.1.4$locations$ID
print(I.col.1.4)

par(mfrow=c(1,1))
plot(denver.model.1.4$locations$long,
     denver.model.1.4$locations$lat,
     pch=23+floor(I.col.1.4/max(I.col.1.4)+.5), bg=I.col.1.4,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.1.4.cv <- coef(est.denver.model.1.4, pars="cov")[,c("par","init")]
x.init.1.4.cv
```

Run the model with cross validation.
```{r}
est.denver.1.4.cv <- estimateCV(denver.model.1.4, x.init.1.4.cv, Ind.cv.1.4)
print(est.denver.1.4.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.1.4, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.1.4.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data, denver.model.1.4, est.denver.model.1.4, est.denver.1.4.cv,
     file = here::here("Results", "Denver_ST_Model_1.4.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.1.4.cv <- predictCV(denver.model.1.4, est.denver.1.4.cv, LTA = T)
pred.1.4.cv.log <- predictCV(denver.model.1.4, est.denver.1.4.cv,
                             LTA = T, transform="unbiased")

names(pred.1.4.cv)
summary(pred.1.4.cv)
summary(pred.1.4.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod1.4.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 1.5: Smoothing for beta fields and error term

### Create the model object (exp covariance structure)
Now I'm using the ```exp``` covariance structure for ```cov.beta``` (beta0 and beta1) and ```cov.nu```. 

```{r}
names(denver.data$covars)

LUR1.5 <- list(covar_fun, covar_fun)

cov.beta1.5 <-  list(covf=c("exp", "exp"), nugget = c(TRUE, TRUE))
cov.nu1.5 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations1.5 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.1.5 <- createSTmodel(denver.data, LUR = LUR1.5,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta1.5, cov.nu = cov.nu1.5,
                                  locations = locations1.5)
denver.model.1.5
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.1.5, all=FALSE)
names

# x.init.1.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, 0, -1, -5, 0, -1, -5),
#                     c(0, -5, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -5, -5, 0, -1, -5, 0, -1, -5),
#                     c(2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, 2, -1, -5, 2, -1, -5),
#                     c(2, -5, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -5, -5, 2, -1, -5, 2, -1, -5),
#                     c(4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, 4, -1, -5, 4, -1, -5),
#                     c(4, -5, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, 6, -1, -5, 6, -1, -5),
#                     c(6, -5, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -5, -5, 6, -1, -5, 6, -1, -5)
#                     )

x.init.1.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(4, -10, -5, 4, -10, -5, 4, -1, -5),
                    c(4, -5, -1, 4, -10, -1, 4, -1, -1),
                    c(4, -5, -5, 4, -10, -5, 4, -1, -5),
                    c(6, -10, -10, 6, -10, -10, 10, -1, -1),
                    c(6, -10, -5, 6, -10, -5, 10, -1, -5),
                    c(6, -10, -5, 6, -10, -5, 10, -1, -5)
                    )

rownames(x.init.1.5) <- loglikeSTnames(denver.model.1.5, all=FALSE)
x.init.1.5

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.1.5 <- estimate.STmodel(denver.model.1.5, x.init.1.5)
print(est.denver.model.1.5)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.1.5 <- createCV(denver.model.1.5, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.1.5 <- sapply(split(denver.model.1.5$obs$ID, Ind.cv.1.5), unique)
print(sapply(ID.cv.1.5, length))
table(Ind.cv.1.5)

I.col.1.5 <- apply(sapply(ID.cv.1.5,function(x) denver.model.1.5$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.1.5) <- denver.model.1.5$locations$ID
print(I.col.1.5)

par(mfrow=c(1,1))
plot(denver.model.1.5$locations$long,
     denver.model.1.5$locations$lat,
     pch=23+floor(I.col.1.5/max(I.col.1.5)+.5), bg=I.col.1.5,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.1.5.cv <- coef(est.denver.model.1.5, pars="cov")[,c("par","init")]
x.init.1.5.cv
```

Run the model with cross validation.
```{r}

est.denver.1.5.cv <- estimateCV(denver.model.1.5, x.init.1.5.cv, Ind.cv.1.5)
print(est.denver.1.5.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.1.5, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.1.5.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data, denver.model.1.5, est.denver.model.1.5, est.denver.1.5.cv,
     file = here::here("Results", "Denver_ST_Model_1.5.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.1.5.cv <- predictCV(denver.model.1.5, est.denver.1.5.cv, LTA = T)
pred.1.5.cv.log <- predictCV(denver.model.1.5, est.denver.1.5.cv,
                             LTA = T, transform="unbiased")

names(pred.1.5.cv)
summary(pred.1.5.cv)
summary(pred.1.5.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod1.5.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.1.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.1.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

# Expand the time trends

Based on feedback from Josh, I'm going to try to use the long term record for NO~2~ data from the central site monitors and the BC data from the near-road monitoring site to estimate the spatial trend. Then I'll add this trend to the STdata object

## Update the Denver data set
First, in order to get these models to work, we need to have ST covaraiates that cover the same time frame. Here I'm reading in the updated data sets and getting them into the right shape for the ``SpatioTemporal``` package.

```{r}

data_name <- "Combined_Filter_Data_AEA.csv"
lur_data1.2 <- read_csv(here::here("Data", data_name)) %>%
  filter(!is.na(lon)) %>%
  filter(indoor == 0) %>%
  filter(bc_ug_m3_dem > 0) %>% 
  filter(is.na(below_lod) | below_lod == 0) %>% 
  filter(is.na(low_volume_flag) | low_volume_flag == 0) %>% 
  filter(is.na(flow_rate_flag) | flow_rate_flag == 0) %>% 
  filter(is.na(is_blank) | is_blank == 0) %>% 
  filter(is.na(negative_pm_mass) | negative_pm_mass == 0) %>% 
  filter(is.na(potential_contamination) | potential_contamination == 0) %>% 
  filter(is.na(bc_mass_ug_corrected) | bc_mass_ug_corrected >= 0.49) %>% 
  filter(campaign %in% paste0("Campaign", c(1, 2, 3, "X")))

lur_data1.2 <- lur_data1.2 %>%
  rename("pm_ug_m3_raw" = "pm_ug_m3") %>%
  rename("pm_ug_m3" = "pm_ug_m3_dem") %>%
  rename("bc_ug_m3_raw" = "bc_ug_m3") %>%
  rename("bc_ug_m3" = "bc_ug_m3_dem")

#' Format the date variable to be Year-WeekNo. (starts on Sunday)
#' Convert that back to the first day of the week
lur_data1.2 <- lur_data1.2 %>%
  mutate(sample_week_no = format(StartDateLocal, "%Y-%W")) %>%
  mutate(sample_week = as.Date(cut(as.Date(StartDateLocal), "week")))

#' Add a unique site ID
ids <- read_csv(here::here("Data", "ST_Model_Site_IDs.csv"))

lur_data1.2 <- mutate(lur_data1.2, site_id_lonlat = paste(lon, lat, sep = "_"))
lur_data1.2 <- left_join(lur_data1.2, ids, by = "site_id_lonlat")

lur_data1.2$site_id <- ifelse(lur_data1.2$filter_id == "080310027", "central", lur_data1.2$site_id)
# lur_data1.2$site_id <- ifelse(lur_data1.2$site_id == "central", lur_data1.2$site_id, paste0("d_", lur_data1.2$site_id))

#' Drop old ST covariates
#' Keep linking variables (sample_week and site_id_lonlat)
lur_data2.2 <- select(lur_data1.2, -c(StartDateLocal:units_smoke))

#' Get the new ST covariates
st_covariates_file_name <- "ST_Covariates_Filters_LongTerm_AEA.csv"
st_covariates <- read_csv(here::here("Data", st_covariates_file_name)) %>% 
  mutate(filter_id = as.character(filter_id)) %>% 
  rename("site_id_lonlat" = "site_id",
         "sample_week" = "week") %>% 
  filter(campaign %in% paste0("Campaign", c(1, 2, 3, "X")))

st_covariates <- st_covariates %>% 
  mutate(site_id_lonlat = ifelse(site_id_lonlat == "-104.9440188_39.9372463999999",
                                 "-104.9440188_39.9372464", site_id_lonlat)) %>% 
  mutate(site_id_lonlat = ifelse(site_id_lonlat == "-104.8374204_39.5970357000001",
                                 "-104.8374204_39.5970357", site_id_lonlat))

st_covariates <- st_covariates %>% 
  filter(site_id_lonlat %in% unique(lur_data2.2$site_id_lonlat)) %>% 
  left_join(ids, by = "site_id_lonlat") %>% 
  mutate(site_id = ifelse(filter_id == "080310027", "central", site_id)) %>% 
  # mutate(site_id = ifelse(site_id == "central", site_id, paste0("d_", site_id))) %>% 
  select(site_id, sample_week, nn_pm:units_smoke) %>%
  distinct()

st_cov_dates <- select(st_covariates, site_id, sample_week) %>% 
  distinct()
```

```{r}
#' bc_obs2 with the additional ST dates
bc_obs2a <- lur_data2.2 %>%
  select(site_id, sample_week, bc_ug_m3) %>%
  mutate(log_bc = log(bc_ug_m3)) %>%
  select(-bc_ug_m3) 

#' Fill in the ST dates with NAs
bc_obs2 <- left_join(st_cov_dates, bc_obs2a, by = c("sample_week", "site_id"))

#' Get it into shape!
bc_obs2 <- bc_obs2 %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = log_bc) %>%
              # names_from = site_id, values_from = bc_ug_m3) %>%
  as.data.frame() %>%
  arrange(sample_week)
rownames(bc_obs2) <- bc_obs2$sample_week
bc_obs2$sample_week <- NULL
bc_obs2 <- as.matrix(bc_obs2)

bc_weeks2 <- rownames(bc_obs2)

#' bc_st_no22
bc_st_no22 <- select(st_covariates, site_id, sample_week, idw_no2) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = idw_no2) %>%
  as.data.frame()
rownames(bc_st_no22) <- bc_st_no22$sample_week
bc_st_no22$sample_week <- NULL
bc_st_no22 <- as.matrix(bc_st_no22)

no2_weeks2 <- rownames(bc_st_no22)

setdiff(bc_weeks2, no2_weeks2)

#' smoke days (saving for later)
bc_st_smk2 <- select(st_covariates, site_id, sample_week, area_smoke_2sd) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = area_smoke_2sd) %>%
  as.data.frame()
rownames(bc_st_smk2) <- bc_st_smk2$sample_week
bc_st_smk2$sample_week <- NULL
bc_st_smk2 <- as.matrix(bc_st_smk2)

smk_weeks2 <- rownames(bc_st_smk2)

setdiff(bc_weeks2, smk_weeks2)
```

Second, create a new ```denver.data``` object. Note at the bc_sp_cov object wasn't updated because these variables don't change with time.
```{r}
denver.data2 <- createSTdata(obs = bc_obs2,
                             covars = bc_sp_cov,
                             SpatioTemporal = list(bc_st_no2 = bc_st_no22))
D2 <- createDataMatrix(denver.data2)
denver.data2
```

## Creating the NO2 data objects
Combing NO2 and BC data (scaled)
```{r, include = F}
#' Denver Metro area counties
counties <- c("001", "005", "013", "014", "031", "059")

#' NO2 concentrations
no2_data <- read_csv(here::here("Data", "Monitor_NO2_Data_AEA.csv")) %>%
  # filter(!is.na(Arithmetic_Mean)) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  select(-year) %>% 
  filter(County_Code %in% counties) %>%
  arrange(Date_Local, monitor_id)

#' NO2 as a wide DF
no2_obs <- no2_data %>% 
  select(monitor_id, Date_Local, Arithmetic_Mean) %>%
  mutate(sample_week = as.character(as.Date(cut(as.Date(Date_Local), "week")))) %>%
  filter(sample_week %in% bc_weeks2) %>% 
  group_by(monitor_id, sample_week) %>%
  summarize(no2 = mean(Arithmetic_Mean, na.rm=T)) %>%
  pivot_wider(id_cols = sample_week,
              names_from = monitor_id, values_from = no2) %>%
  as.data.frame() %>%
  arrange(sample_week)

#' Drop monitor 080310027 because we have BC measured at this site
no2_obs[,"080310027"] <- NULL
# colnames(no2_obs)[5] <- "080310027a" 

#' BC concentrations
bc_data <- read_csv(here::here("Data", "Monitor_BC_Data_AEA.csv")) %>%
  # filter(!is.na(Arithmetic_Mean)) %>%
  filter(County_Code %in% counties) %>%
  dplyr::select(monitor_id, Date_Local, Arithmetic_Mean) %>% 
  group_by(monitor_id, Date_Local) %>% 
  summarize(Arithmetic_Mean = mean(Arithmetic_Mean, na.rm = T))

#' BC as a wide DF
bc_obs_temp <- bc_data %>% 
  select(monitor_id, Date_Local, Arithmetic_Mean) %>%
  mutate(sample_week = as.character(as.Date(cut(as.Date(Date_Local), "week")))) %>%
  filter(sample_week %in% bc_weeks2) %>% 
  group_by(monitor_id, sample_week) %>%
  summarize(bc = mean(Arithmetic_Mean, na.rm=T)) %>%
  pivot_wider(id_cols = sample_week,
              names_from = monitor_id, values_from = bc) %>%
  as.data.frame() %>%
  arrange(sample_week)

#' Combine them
no2_bc_obs <- left_join(no2_obs, bc_obs_temp, by = "sample_week")

#' Scale the measurements to avoid issues where units are different
rownames(no2_bc_obs) <- no2_bc_obs$sample_week
no2_bc_obs$sample_week <- NULL
no2_bc_obs <- as.matrix(no2_bc_obs)

rnames <- rownames(no2_bc_obs)
cnames <- colnames(no2_bc_obs)

#' Scale the data by columns (monitors)
no2_bc_obs <- apply(no2_bc_obs, 2, scale)

rownames(no2_bc_obs) <- rnames
colnames(no2_bc_obs) <- cnames

head(rownames(no2_bc_obs))
head(colnames(no2_bc_obs))
class(no2_bc_obs)
dim(no2_bc_obs)
```

Get the spatial information together
```{r}

# NO2 SP object
no2_data2 <- read_csv(here::here("Data", "Monitor_NO2_Data_AEA.csv")) %>%
  # filter(!is.na(Arithmetic_Mean)) %>%
  filter(Date_Local < as.Date(rownames(bc_obs2)[nrow(bc_obs2)])) %>% 
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  select(-year) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  # mutate(monitor_id = ifelse(monitor_id == "080310027", "080310027a", monitor_id)) %>% 
  filter(monitor_id %in% colnames(no2_obs))

no2_coords <- do.call(rbind, st_geometry(no2_data2)) %>%
  as_tibble() %>% setNames(c("x","y"))
no2_sp_lonlat <- no2_data2 %>%
  st_transform(crs = ll_wgs84)
no2_coords2 <- do.call(rbind, st_geometry(no2_sp_lonlat)) %>%
  as_tibble() %>% setNames(c("lon","lat"))

no2_sp_cov <- st_set_geometry(no2_data2, NULL) %>%
  select(monitor_id) %>%
  rename(ID = monitor_id)
no2_sp_cov <- bind_cols(no2_sp_cov, no2_coords, no2_coords2) %>%
  as.data.frame() %>%
  distinct()

#' BC sp cov
bc_sp_cov2 <- select(bc_sp_cov, colnames(no2_sp_cov)) %>% 
  filter(ID == "central") %>% 
  mutate(ID = ifelse(ID == "central", "080310027", ID))
head(bc_sp_cov2)

no2_bc_sp_cov <- bind_rows(no2_sp_cov, bc_sp_cov2)

no2_bc_STdata <- createSTdata(obs = no2_bc_obs,
                              covars = no2_bc_sp_cov)
print(no2_bc_STdata)

```

## Generate the temporal trend 

```{r}
D_no2 <- createDataMatrix(no2_bc_STdata)
# D_no2

unique(as.Date(cut(as.Date(no2_data2$Date_Local), "year")))
n_years <- length(unique(as.Date(cut(as.Date(no2_data2$Date_Local), "year"))))
n_years

n_years * 4

no2.SVD.cv.4py <- SVDsmoothCV(D_no2, 0:4, df = 44)
print(no2.SVD.cv.4py)
plot(no2.SVD.cv.4py)
```

## Plot the time trends

I tried using 1 and 2 basis functions for the time trends:

First, see what the time trends look like with 1 basis function
```{r}
no2_bc_STdata1 <- updateTrend(no2_bc_STdata, n.basis = 1, df = 44)
no2_bc_STdata1

head(no2_bc_STdata1$trend)
plot(no2_bc_STdata1$trend$date, no2_bc_STdata1$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
# points(no2_bc_STdata1$trend$date, no2_bc_STdata1$trend$V2, col = 2, pch = 16, cex = 0.5)
lines(no2_bc_STdata1$trend$date, no2_bc_STdata1$trend$V1, col = 1)
# lines(no2_bc_STdata1$trend$date, no2_bc_STdata1$trend$V2, col = 2)
```

Next, let's try 2 basis functions
```{r}
no2_bc_STdata2 <- updateTrend(no2_bc_STdata, n.basis = 2, df = 44)
no2_bc_STdata2

head(no2_bc_STdata2$trend)
plot(no2_bc_STdata2$trend$date, no2_bc_STdata2$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
points(no2_bc_STdata2$trend$date, no2_bc_STdata2$trend$V2, col = 2, pch = 16, cex = 0.5)
lines(no2_bc_STdata2$trend$date, no2_bc_STdata2$trend$V1, col = 1)
lines(no2_bc_STdata2$trend$date, no2_bc_STdata2$trend$V2, col = 2)
```

## Update the new Denver dataset

I'm replacing the ```denver.data2``` trend with the one estimated for the NO2 and BC data

Start with 1 basis function:
```{r}
denver.data2.1 <- denver.data2
denver.data2.1$trend <- no2_bc_STdata1$trend
print(denver.data2.1)
```

Then, 2 basis function:
```{r}
denver.data2.2 <- denver.data2
denver.data2.2$trend <- no2_bc_STdata2$trend
print(denver.data2.2)
```

## Evaluate the basis functions 
Here I'm using using the NO~2~ + BC data instead of the BC data.
Plotting the temporal trends at a few of the sites. These are the same locations as plotted above.

Comparing the plots using one basis function vs. two basis functions, it looks like using two basis functions might be better than 1. There was some residual autocorrelation in the central site monitor data when using just one basis function (see the fourth plot in the first series of plots below). Using two basis functions helped to reduce this autocorrelation and improved fits for the other locations as well.

**One basis function**:
```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.1, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_1")
plot(denver.data2.1, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.1, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_20")
plot(denver.data2.1, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.1, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_53")
plot(denver.data2.1, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.1, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="central")
plot(denver.data2.1, "pacf", ID="central")
```

**Two basis functions**:
```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.2, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_1")
plot(denver.data2.2, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.2, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_20")
plot(denver.data2.2, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.2, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_53")
plot(denver.data2.2, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.2, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="central")
plot(denver.data2.2, "pacf", ID="central")
```

# Updated models (NO2 + BC, two basis functions)
These modes are based on the expanded time trend data, and here we are using 2 temporal basis functions

## Model 2.1: simplest form 

### Create the model object 
For this version of the model, use ```iid``` for both ```cov.beta``` (beta0, beta1, beta2) and ```cov.nu``` (error term).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.2$covars)

LUR2.1 <- list(covar_fun, covar_fun, covar_fun)

cov.beta2.1 <-  list(covf="iid", nugget = T)
cov.nu2.1 <- list(covf="iid", nugget = T, random.effect = FALSE)
locations2.1 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.2.1 <- createSTmodel(denver.data2.2, LUR = LUR2.1,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta2.1, cov.nu = cov.nu2.1,
                                  locations = locations2.1)
denver.model.2.1
```

### Estimate model parameters

We want to try a number of initial conditions to make sure we find the "right" solution.

```{r}
names <- loglikeSTnames(denver.model.2.1, all=FALSE)
names

# x.init.2.1 <- cbind(c(rep(-1, 4)), c(rep(-2, 4)), c(rep(-3, 4)),
#                     c(rep(-4, 4)), c(rep(-5, 4)), c(rep(-6, 4)))
# x.init.2.1[nrow(x.init.2.1),] <- 0

x.init.2.1 <- cbind(c(0, 0, 0),
                    c(-5, -5, -5, -5),
                    c(-8, -5, -5, -5),
                    c(-8, -8, -5, -5),
                    c(-8, -8, -8, -8))

rownames(x.init.2.1) <- loglikeSTnames(denver.model.2.1, all=FALSE)
x.init.2.1

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.2.1 <- estimate.STmodel(denver.model.2.1, x.init.2.1)
print(est.denver.model.2.1)
```

### Cross-validation

Define the CV groups (and don't forget to set the seed here!)
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.2.1 <- createCV(denver.model.2.1, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.2.1 <- sapply(split(denver.model.2.1$obs$ID, Ind.cv.2.1), unique)
print(sapply(ID.cv.2.1, length))
table(Ind.cv.2.1)

I.col.2.1 <- apply(sapply(ID.cv.2.1,function(x) denver.model.2.1$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.2.1) <- denver.model.2.1$locations$ID
print(I.col.2.1)

par(mfrow=c(1,1))
plot(denver.model.2.1$locations$long,
     denver.model.2.1$locations$lat,
     pch=23+floor(I.col.2.1/max(I.col.2.1)+.5), bg=I.col.2.1,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.2.1.cv <- coef(est.denver.model.2.1, pars="cov")[,c("par","init")]
x.init.2.1.cv
```

Run the model with cross validation.
```{r}
est.denver.2.1.cv <- estimateCV(denver.model.2.1, x.init.2.1.cv, Ind.cv.2.1)
print(est.denver.2.1.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.2.1, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.2.1.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.2, denver.model.2.1, est.denver.model.2.1, est.denver.2.1.cv,
     file = here::here("Results", "Denver_ST_Model_2.1.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.2.1.cv <- predictCV(denver.model.2.1, est.denver.2.1.cv, LTA = T)
pred.2.1.cv.log <- predictCV(denver.model.2.1, est.denver.2.1.cv,
                             LTA = T, transform="unbiased")

names(pred.2.1.cv)
summary(pred.2.1.cv)
summary(pred.2.1.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod2.1.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 2.2: Smoothing in the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta, beta1, beta2) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.2$covars)

LUR2.2 <- list(covar_fun, covar_fun, covar_fun)

cov.beta2.2 <-  list(covf = c("iid", "iid", "iid"), nugget = T)
cov.nu2.2 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations2.2 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.2.2 <- createSTmodel(denver.data2.2, LUR = LUR2.2,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta2.2, cov.nu = cov.nu2.2,
                                  locations = locations2.2)
denver.model.2.2
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.2.2, all=FALSE)
names

# x.init.2.2 <- cbind(c(0, 0, 0, 0, 0, 0),
#                     c(-1, -1, -1, 0, -1, -1),
#                     c(-1, -1, -1, 0, -5, -1),
#                     c(-5, -5, -5, 0, -1, -5),
#                     c(-5, -5, -5, 0, -5, -5),
#                     c(-1, -1, -1, 2, -1, -1),
#                     c(-1, -1, -1, 2, -5, -1),
#                     c(-5, -5, -5, 2, -1, -5),
#                     c(-5, -5, -5, 2, -5, -5),
#                     c(-1, -1, -1, 4, -1, -1),
#                     c(-1, -1, -1, 4, -5, -1),
#                     c(-5, -5, -5, 4, -1, -5),
#                     c(-5, -5, -5, 4, -5, -5))

x.init.2.2 <- cbind(c(0, 0, 0, 0, 0, 0),
                    c(-10, -5, -5, 4, -3, -5),
                    c(-10, -5, -5, 4, -5, -5),
                    c(-10, -5, -5, 6, -3, -5),
                    c(-10, -5, -5, 6, -5, -5),
                    c(-10, -5, -5, 8, -3, -5),
                    c(-10, -5, -5, 8, -5, -5),
                    c(-10, -5, -5, 10, -3, -5),
                    c(-10, -5, -5, 10, -5, -5))

rownames(x.init.2.2) <- loglikeSTnames(denver.model.2.2, all=FALSE)
x.init.2.2

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.2.2 <- estimate.STmodel(denver.model.2.2, x.init.2.2)
print(est.denver.model.2.2)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.2.2 <- createCV(denver.model.2.2, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.2.2 <- sapply(split(denver.model.2.2$obs$ID, Ind.cv.2.2), unique)
print(sapply(ID.cv.2.2, length))
table(Ind.cv.2.2)

I.col.2.2 <- apply(sapply(ID.cv.2.2, function(x) denver.model.2.2$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.2.2) <- denver.model.2.2$locations$ID
print(I.col.2.2)

par(mfrow=c(1,1))
plot(denver.model.2.2$locations$long,
     denver.model.2.2$locations$lat,
     pch=23+floor(I.col.2.2/max(I.col.2.2)+.5), bg=I.col.2.2,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.2.2.cv <- coef(est.denver.model.2.2, pars="cov")[,c("par","init")]
x.init.2.2.cv
```

Run the model with cross validation
```{r}

est.denver.2.2.cv <- estimateCV(denver.model.2.2, x.init.2.2.cv, Ind.cv.2.2)
print(est.denver.2.2.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.2.2, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.2.2.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.2, denver.model.2.2, est.denver.model.2.2, est.denver.2.2.cv,
     file = here::here("Results", "Denver_ST_Model_2.2.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.2.2.cv <- predictCV(denver.model.2.2, est.denver.2.2.cv, LTA = T)
pred.2.2.cv.log <- predictCV(denver.model.2.2, est.denver.2.2.cv,
                             LTA = T, transform="unbiased")

names(pred.2.2.cv)
summary(pred.2.2.cv)
summary(pred.2.2.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod2.2.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 2.3: Smoothing in beta0 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta1, beta2) and ```exp``` for ```cov.beta``` (beta0) and  ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.2$covars)

LUR2.3 <- list(covar_fun, covar_fun, covar_fun)

cov.beta2.3 <-  list(covf = c("exp", "iid", "iid"), nugget = c(T, T, T))
cov.nu2.3 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations2.3 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.2.3 <- createSTmodel(denver.data2.2, LUR = LUR2.3,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta2.3, cov.nu = cov.nu2.3,
                                  locations = locations2.3)
denver.model.2.3
```

### Estimate model parameters

Josh gave some guidance on how to set up the initial values (NOTE: the ```iid``` model doesn't have range or sill values, but the ```exp``` model does):

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.2.3, all=FALSE)
names

# x.init.2.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, -5, -5, 0, -1, -1),
#                     c(0, -5, -1, -1, -1, 0, -5, -5),
#                     c(0, -5, -5, -5, -5, 0, -5, -5),
#                     c(2, -1, -1, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, -5, -5, 2, -1, -1),
#                     c(2, -5, -1, -1, -1, 2, -5, -5),
#                     c(2, -5, -5, -5, -5, 2, -5, -5),
#                     c(4, -1, -1, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, -5, -5, 4, -1, -1),
#                     c(4, -5, -1, -1, -1, 4, -5, -5),
#                     c(4, -5, -5, -5, -5, 4, -5, -5),
#                     c(6, -1, -1, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, -5, -5, 6, -1, -1),
#                     c(6, -5, -1, -1, -1, 6, -5, -5),
#                     c(6, -5, -5, -5, -5, 6, -5, -5)
#                     )
# x.init.2.3[nrow(x.init.2.3),] <- 0

x.init.2.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
                    c(4, -5, -5, -5, -5, 4, -5, -5),
                    c(4, -5, -5, -5, -5, 6, -5, -5),
                    c(4, -5, -5, -5, -5, 8, -5, -5),
                    c(4, -5, -5, -5, -5, 8, -5, -5),
                    c(4, -10, -10, -5, -5, 4, -5, -5),
                    c(4, -10, -10, -5, -5, 6, -5, -5),
                    c(4, -10, -10, -5, -5, 8, -5, -5),
                    c(4, -10, -10, -5, -5, 10, -5, -5)
                    )

rownames(x.init.2.3) <- loglikeSTnames(denver.model.2.3, all=FALSE)
x.init.2.3

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.2.3 <- estimate.STmodel(denver.model.2.3, x.init.2.3)
print(est.denver.model.2.3)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.2.3 <- createCV(denver.model.2.3, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.2.3 <- sapply(split(denver.model.2.3$obs$ID, Ind.cv.2.3), unique)
print(sapply(ID.cv.2.3, length))
table(Ind.cv.2.3)

I.col.2.3 <- apply(sapply(ID.cv.2.3, function(x) denver.model.2.3$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.2.3) <- denver.model.2.3$locations$ID
print(I.col.2.3)

par(mfrow=c(1,1))
plot(denver.model.2.3$locations$long,
     denver.model.2.3$locations$lat,
     pch=23+floor(I.col.2.3/max(I.col.2.3)+.5), bg=I.col.2.3,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.2.3.cv <- coef(est.denver.model.2.3, pars="cov")[,c("par","init")]
x.init.2.3.cv
```

Run the model with cross validation
```{r}

est.denver.2.3.cv <- estimateCV(denver.model.2.3, x.init.2.3.cv, Ind.cv.2.3)
print(est.denver.2.3.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.2.3, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.2.3.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.2, denver.model.2.3, est.denver.model.2.3, est.denver.2.3.cv,
     file = here::here("Results", "Denver_ST_Model_2.3.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.2.3.cv <- predictCV(denver.model.2.3, est.denver.2.3.cv, LTA = T)
pred.2.3.cv.log <- predictCV(denver.model.2.3, est.denver.2.3.cv,
                             LTA = T, transform="unbiased")

names(pred.2.3.cv)
summary(pred.2.3.cv)
summary(pred.2.3.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod2.3.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 2.4: Smoothing for beta1, beta2 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta0) and ```exp``` for ```cov.beta``` (beta1, beta2) ```cov.nu``` (error).

```{r}
names(denver.data2.2$covars)

LUR2.4 <- list(covar_fun, covar_fun, covar_fun)

cov.beta2.4 <-  list(covf=c("iid", "exp", "exp"), nugget = c(TRUE, TRUE, TRUE))
cov.nu2.4 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations2.4 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.2.4 <- createSTmodel(denver.data2.2, LUR = LUR2.4,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta2.4, cov.nu = cov.nu2.4,
                                  locations = locations2.4)
denver.model.2.4
```

### Estimate model parameters 

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.2.4, all=FALSE)
names
# 
# x.init.2.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(-1, 0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(-1, 0, -1, -5, 0, -5, -1, 0, -5, -1),
#                     c(-5, 0, -5, -1, 0, -1, -5, 0, -1, -5),
#                     c(-5, 0, -5, -5, 0, -5, -5, 0, -5, -5),
#                     c(-1, 2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(-1, 2, -1, -5, 2, -5, -1, 2, -5, -1),
#                     c(-5, 2, -5, -1, 2, -1, -5, 2, -1, -5),
#                     c(-5, 2, -5, -5, 2, -5, -5, 2, -5, -5),
#                     c(-1, 4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(-1, 4, -1, -5, 4, -5, -1, 4, -5, -1),
#                     c(-5, 4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(-5, 4, -5, -5, 4, -5, -5, 4, -5, -5),
#                     c(-1, 6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(-1, 6, -1, -5, 6, -5, -1, 6, -5, -1),
#                     c(-5, 6, -5, -5, 6, -1, -5, 6, -1, -5),
#                     c(-5, 6, -5, -5, 6, -5, -5, 6, -5, -5))
# x.init.2.4[nrow(x.init.2.4),] <- 0

x.init.2.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(-5, 4, -5, -5, 6, -5, -5, 6, -5, -5),
                    c(-5, 4, -5, -5, 6, -5, -5, 6, -5, -5),
                    c(-10, 4, -10, -1, 6, -5, -10, 10, -5, -5),
                    c(-10, 4, -10, -5, 6, -5, -10, 10 -5, -5),
                    c(-13, 4, -10, -1, 6, -5, -13, 10, -5, -5),
                    c(-13, 4, -10, -5, 6, -5, -13, 10, -5, -5))

rownames(x.init.2.4) <- loglikeSTnames(denver.model.2.4, all=FALSE)
x.init.2.4

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.2.4 <- estimate.STmodel(denver.model.2.4, x.init.2.4)
print(est.denver.model.2.4)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.2.4 <- createCV(denver.model.2.4, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.2.4 <- sapply(split(denver.model.2.4$obs$ID, Ind.cv.2.4), unique)
print(sapply(ID.cv.2.4, length))
table(Ind.cv.2.4)

I.col.2.4 <- apply(sapply(ID.cv.2.4,function(x) denver.model.2.4$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.2.4) <- denver.model.2.4$locations$ID
print(I.col.2.4)

par(mfrow=c(1,1))
plot(denver.model.2.4$locations$long,
     denver.model.2.4$locations$lat,
     pch=23+floor(I.col.2.4/max(I.col.2.4)+.5), bg=I.col.2.4,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.2.4.cv <- coef(est.denver.model.2.4, pars="cov")[,c("par","init")]
x.init.2.4.cv
```

Run the model with cross validation.
```{r}
est.denver.2.4.cv <- estimateCV(denver.model.2.4, x.init.2.4.cv, Ind.cv.2.4)
print(est.denver.2.4.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.2.4, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.2.4.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.2, denver.model.2.4, est.denver.model.2.4, est.denver.2.4.cv,
     file = here::here("Results", "Denver_ST_Model_2.4.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.2.4.cv <- predictCV(denver.model.2.4, est.denver.2.4.cv, LTA = T)
pred.2.4.cv.log <- predictCV(denver.model.2.4, est.denver.2.4.cv,
                             LTA = T, transform="unbiased")

names(pred.2.4.cv)
summary(pred.2.4.cv)
summary(pred.2.4.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod2.4.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.2.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.2.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```



## Model 2.5: Smoothing for beta fields and error term

### Create the model object (exp covariance structure)

Now I'm using the ```exp``` covariance structure for ```cov.beta``` and ```cov.nu```.

```{r}
names(denver.data2.2$covars)

LUR2.5<- list(covar_fun, covar_fun, covar_fun)

cov.beta2.5 <-  list(covf=c("exp", "exp", "exp"), nugget = c(TRUE, TRUE, TRUE))
cov.nu2.5 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations2.5 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.2.5 <- createSTmodel(denver.data2.2, LUR = LUR2.5,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta2.5, cov.nu = cov.nu2.5,
                                  locations = locations2.5)
denver.model.2.5
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.2.5, all=FALSE)
names

# x.init.2.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, 0, -1, -5, 0, -1, -5, 0, -1, -5),
#                     c(0, -5, -1, 0, -5, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -5, -5, 0, -5, -5, 0, -1, -5, 0, -1, -5),
#                     c(2, -1, -1, 2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, 2, -1, -5, 2, -1, -5, 2, -1, -5),
#                     c(2, -5, -1, 2, -5, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -5, -5, 2, -5, -5, 2, -1, -5, 2, -1, -5),
#                     c(4, -1, -1, 4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, 4, -1, -5, 4, -1, -5, 4, -1, -5),
#                     c(4, -5, -1, 4, -5, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -5, -5, 4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(6, -1, -1, 6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, 6, -1, -5, 6, -1, -5, 6, -1, -5),
#                     c(6, -5, -1, 6, -5, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -5, -5, 6, -5, -5, 6, -1, -5, 6, -1, -5)
#                     )

x.init.2.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(6, -5, -1, 6, -5, -1, 6, -1, -1, 6, -1, -1),
                    c(6, -5, -5, 6, -5, -5, 6, -5, -10, 10, -5, -5),
                    c(6, -10, -1, 6, -10, -1, 6, -1, -1, 6, -1, -1),
                    c(6, -10, -5, 6, -10, -5, 6, -5, -5, 6, -5, -5))

rownames(x.init.2.5) <- loglikeSTnames(denver.model.2.5, all=FALSE)
x.init.2.5

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.2.5 <- estimate.STmodel(denver.model.2.5, x.init.2.5)
print(est.denver.model.2.5)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.2.5 <- createCV(denver.model.2.5, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.2.5 <- sapply(split(denver.model.2.5$obs$ID, Ind.cv.2.5), unique)
print(sapply(ID.cv.2.5, length))
table(Ind.cv.2.5)

I.col.2.5 <- apply(sapply(ID.cv.2.5,function(x) denver.model.2.5$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.2.5) <- denver.model.2.5$locations$ID
print(I.col.2.5)

par(mfrow=c(1,1))
plot(denver.model.2.5$locations$long,
     denver.model.2.5$locations$lat,
     pch=23+floor(I.col.2.5/max(I.col.2.5)+.5), bg=I.col.2.5,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.2.5.cv <- coef(est.denver.model.2.5, pars="cov")[,c("par","init")]
x.init.2.5.cv
```

Run the model with cross validation.

Note: I got the following error when trying to run the CV:

Error in solve.default(res[[i]]$hessian) : 
  Lapack routine dgesv: system is exactly singular: U[4,4] = 0
```{r}

# est.denver.2.5.cv <- estimateCV(denver.model.2.5, x.init.2.5.cv, Ind.cv.2.5)
# print(est.denver.2.5.cv)
# 
# par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
# with(coef(est.denver.model.2.5, pars="all"),
#      plotCI((1:length(par))+.3, par, uiw=1.96*sd,
#             col=2, xlab="", xaxt="n", ylab=""))
# boxplot(est.denver.2.5.cv, "all", boxwex=.4, col="grey", add=TRUE)
# 
# #' Save the results as an .rdata object
# save(denver.data2.2, denver.model.2.5, est.denver.model.2.5, est.denver.2.5.cv,
#      file = here::here("Results", "Denver_ST_Model_2.5.rdata"))
```

### Prediction using the CV model
<!-- Making predictions using the CV model. Printing out the CV summary statistics as well-->

I didn't make predictions with the CV model since it wasn't successfully implemented

```{r}
# pred.2.5.cv <- predictCV(denver.model.2.5, est.denver.2.5.cv, LTA = T)
# pred.2.5.cv.log <- predictCV(denver.model.2.5, est.denver.2.5.cv,
#                              LTA = T, transform="unbiased")
# 
# names(pred.2.5.cv)
# summary(pred.2.5.cv)
# summary(pred.2.5.cv.log)
# 
# par(mfrow=c(1,2), mar=c(3.3,3.3,2.5,1), mgp=c(2,1,0))
# plot(pred.2.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.2.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# 
# jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod2.5.jpeg"),
#      width = 8, height = 4, units = "in", res = 500)
# par(mfrow=c(1,2), mar=c(3.3,3.3,2.5,1), mgp=c(2,1,0))
# plot(pred.2.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.2.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# dev.off()
```

# Updated models (NO2 + BC, one basis function)
The models above were OK, but we were hitting the upper limit on a lot of the parameters. Here I though I would try going back to 1 basis function (just to see what happens). I know based on the plots above that using one basis function resulted in a good amount of residual autocorrelation. I'm mostly running these models for my own edification.

## Model 3.1: simplest form 

### Create the model object 
For this version of the model, use ```iid``` for both ```cov.beta``` (beta0 and beta1) and ```cov.nu``` (error term).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.1$covars)

LUR3.1 <- list(covar_fun, covar_fun)

cov.beta3.1 <-  list(covf="iid", nugget = T)
cov.nu3.1 <- list(covf="iid", nugget = T, random.effect = FALSE)
locations3.1 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.3.1 <- createSTmodel(denver.data2.1, LUR = LUR3.1,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta3.1, cov.nu = cov.nu3.1,
                                  locations = locations3.1)
denver.model.3.1
```

### Estimate model parameters

We want to try a number of initial conditions to make sure we find the "right" solution.

```{r}
names <- loglikeSTnames(denver.model.3.1, all=FALSE)
names

# x.init.3.1 <- cbind(c(rep(-1, 3)), c(rep(-2, 3)), c(rep(-3, 3)),
#                     c(rep(-4, 3)), c(rep(-5, 3)), c(rep(-6, 3)))
# x.init.3.1[nrow(x.init.3.1),] <- 0

x.init.3.1 <- cbind(c(0, 0, 0),
                    c(-5, -10, -10),
                    c(-8, -8, -5),
                    c(-8, -5, -5))

rownames(x.init.3.1) <- loglikeSTnames(denver.model.3.1, all=FALSE)
x.init.3.1

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.3.1 <- estimate.STmodel(denver.model.3.1, x.init.3.1)
print(est.denver.model.3.1)
```

### Cross-validation

Define the CV groups (and don't forget to set the seed here!)
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.3.1 <- createCV(denver.model.3.1, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.3.1 <- sapply(split(denver.model.3.1$obs$ID, Ind.cv.3.1), unique)
print(sapply(ID.cv.3.1, length))
table(Ind.cv.3.1)

I.col.3.1 <- apply(sapply(ID.cv.3.1,function(x) denver.model.3.1$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.3.1) <- denver.model.3.1$locations$ID
print(I.col.3.1)

par(mfrow=c(1,1))
plot(denver.model.3.1$locations$long,
     denver.model.3.1$locations$lat,
     pch=23+floor(I.col.3.1/max(I.col.3.1)+.5), bg=I.col.3.1,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.3.1.cv <- coef(est.denver.model.3.1, pars="cov")[,c("par","init")]
x.init.3.1.cv
```

Run the model with cross validation.
```{r}
est.denver.3.1.cv <- estimateCV(denver.model.3.1, x.init.3.1.cv, Ind.cv.3.1)
print(est.denver.3.1.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.3.1, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.3.1.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.1, denver.model.3.1, est.denver.model.3.1, est.denver.3.1.cv,
     file = here::here("Results", "Denver_ST_Model_3.1.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.3.1.cv <- predictCV(denver.model.3.1, est.denver.3.1.cv, LTA = T)
pred.3.1.cv.log <- predictCV(denver.model.3.1, est.denver.3.1.cv,
                             LTA = T, transform="unbiased")

names(pred.3.1.cv)
summary(pred.3.1.cv)
summary(pred.3.1.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod3.1.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 3.2: Smoothing in the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta0 and beta1) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.1$covars)

LUR3.2 <- list(covar_fun, covar_fun)

cov.beta3.2 <-  list(covf = c("iid", "iid"), nugget = T)
cov.nu3.2 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations3.2 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.3.2 <- createSTmodel(denver.data2.1, LUR = LUR3.2,
                                      ST = "bc_st_no2",
                                      cov.beta = cov.beta3.2, cov.nu = cov.nu3.2,
                                      locations = locations3.2)
denver.model.3.2
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.3.2, all=FALSE)
names

# x.init.3.2 <- cbind(c(0, 0, 0, 0, 0),
#                         c(-1, -1, 0, -1, -1),
#                         c(-1, -1, 0, -5, -1),
#                         c(-5, -5, 0, -1, -5),
#                         c(-5, -5, 0, -5, -5),
#                         c(-1, -1, 2, -1, -1),
#                         c(-1, -1, 2, -5, -1),
#                         c(-5, -5, 2, -1, -5),
#                         c(-5, -5, 2, -5, -5),
#                         c(-1, -1, 4, -1, -1),
#                         c(-1, -1, 4, -5, -1),
#                         c(-5, -5, 4, -1, -5),
#                         c(-5, -5, 4, -5, -5))

x.init.3.2 <- cbind(c(0, 0, 0, 0, 0),
                        c(-5, -5, 4, -3, -5),
                        c(-10, -5, 4, -5, -5),
                        c(-5, -5, 6, -3, -5),
                        c(-10, -5, 6, -5, -5),
                        c(-5, -5, 8, -3, -5),
                        c(-10, -5, 8, -5, -5))

rownames(x.init.3.2) <- loglikeSTnames(denver.model.3.2, all=FALSE)
x.init.3.2

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.3.2 <- estimate.STmodel(denver.model.3.2, x.init.3.2)
print(est.denver.model.3.2)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.3.2 <- createCV(denver.model.3.2, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.3.2 <- sapply(split(denver.model.3.2$obs$ID, Ind.cv.3.2), unique)
print(sapply(ID.cv.3.2, length))
table(Ind.cv.3.2)

I.col.3.2 <- apply(sapply(ID.cv.3.2, function(x) denver.model.3.2$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.3.2) <- denver.model.3.2$locations$ID
print(I.col.3.2)

par(mfrow=c(1,1))
plot(denver.model.3.2$locations$long,
     denver.model.3.2$locations$lat,
     pch=23+floor(I.col.3.2/max(I.col.3.2)+.5), bg=I.col.3.2,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.3.2.cv <- coef(est.denver.model.3.2, pars="cov")[,c("par","init")]
x.init.3.2.cv
```

Run the model with cross validation
```{r}

est.denver.3.2.cv <- estimateCV(denver.model.3.2, x.init.3.2.cv, Ind.cv.3.2)
print(est.denver.3.2.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.3.2, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.3.2.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.1, denver.model.3.2, est.denver.model.3.2, est.denver.3.2.cv,
     file = here::here("Results", "Denver_ST_Model_3.2.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.3.2.cv <- predictCV(denver.model.3.2, est.denver.3.2.cv, LTA = T)
pred.3.2.cv.log <- predictCV(denver.model.3.2, est.denver.3.2.cv,
                             LTA = T, transform="unbiased")

names(pred.3.2.cv)
summary(pred.3.2.cv)
summary(pred.3.2.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod3.2.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```


## Model 3.3: Smoothing in beta0 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta1) and ```exp``` for ```cov.beta``` (beta0) ```cov.nu``` (error).

Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data2.1$covars)

LUR3.3 <- list(covar_fun, covar_fun)

cov.beta3.3 <-  list(covf = c("exp", "iid"), nugget = T)
cov.nu3.3 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations3.3 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.3.3 <- createSTmodel(denver.data2.1, LUR = LUR3.3,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta3.3, cov.nu = cov.nu3.3,
                                  locations = locations3.3)
denver.model.3.3
```

### Estimate model parameters

Josh gave some guidance on how to set up the initial values (NOTE: the ```iid``` model doesn't have range or sill values, but the ```exp``` model does):

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.3.3, all=FALSE)
names

# x.init.3.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, -5, 0, -1, -1),
#                     c(0, -5, -1, -1, 0, -5, -5),
#                     c(0, -5, -5, -5, 0, -5, -5),
#                     c(2, -1, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, -5, 2, -1, -1),
#                     c(2, -5, -1, -1, 2, -5, -5),
#                     c(2, -5, -5, -5, 2, -5, -5),
#                     c(4, -1, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, -5, 4, -1, -1),
#                     c(4, -5, -1, -1, 4, -5, -5),
#                     c(4, -5, -5, -5, 4, -5, -5),
#                     c(6, -1, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, -5, 6, -1, -1),
#                     c(6, -5, -1, -1, 6, -5, -5),
#                     c(6, -5, -5, -5, 6, -5, -5)
#                     )
# x.init.3.3[nrow(x.init.3.3),] <- 0

x.init.3.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
                    c(4, -1, -1, -1, 6, -1, -1),
                    c(4, -1, -5, -5, 6, -1, -1),
                    c(4, -10, -10, -1, 6, -3, -5),
                    c(4, -10, -5, -5, 6, -3, -5),
                    c(6, -1, -1, -1, 10, -1, -1),
                    c(6, -1, -5, -5, 10, -1, -1),
                    c(6, -10, -10, -1, 10, -3, -5),
                    c(6, -10, -5, -5, 10, -3, -5)
                    )

rownames(x.init.3.3) <- loglikeSTnames(denver.model.3.3, all=FALSE)
x.init.3.3

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.3.3 <- estimate.STmodel(denver.model.3.3, x.init.3.3)
print(est.denver.model.3.3)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs))
Ind.cv.3.3 <- createCV(denver.model.3.3, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.3.3 <- sapply(split(denver.model.3.3$obs$ID, Ind.cv.3.3), unique)
print(sapply(ID.cv.3.3, length))
table(Ind.cv.3.3)

I.col.3.3 <- apply(sapply(ID.cv.3.3, function(x) denver.model.3.3$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.3.3) <- denver.model.3.3$locations$ID
print(I.col.3.3)

par(mfrow=c(1,1))
plot(denver.model.3.3$locations$long,
     denver.model.3.3$locations$lat,
     pch=23+floor(I.col.3.3/max(I.col.3.3)+.5), bg=I.col.3.3,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.3.3.cv <- coef(est.denver.model.3.3, pars="cov")[,c("par","init")]
x.init.3.3.cv
```

Run the model with cross validation
```{r}

est.denver.3.3.cv <- estimateCV(denver.model.3.3, x.init.3.3.cv, Ind.cv.3.3)
print(est.denver.3.3.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.3.3, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.3.3.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.1, denver.model.3.3, est.denver.model.3.3, est.denver.3.3.cv,
     file = here::here("Results", "Denver_ST_Model_3.3.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.3.3.cv <- predictCV(denver.model.3.3, est.denver.3.3.cv, LTA = T)
pred.3.3.cv.log <- predictCV(denver.model.3.3, est.denver.3.3.cv,
                             LTA = T, transform="unbiased")

names(pred.3.3.cv)
summary(pred.3.3.cv)
summary(pred.3.3.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod3.3.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 3.4: Smoothing for beta1 and error term

For this version of the model, use ```iid``` for ```cov.beta``` (beta0) and ```exp``` for ```cov.beta``` (beta1) ```cov.nu``` (error).

### Create the model object 

```{r}
names(denver.data2.1$covars)

LUR3.4<- list(covar_fun, covar_fun)

cov.beta3.4 <-  list(covf=c("iid", "exp"), nugget = c(TRUE, TRUE))
cov.nu3.4 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations3.4 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.3.4 <- createSTmodel(denver.data2.1, LUR = LUR3.4,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta3.4, cov.nu = cov.nu3.4,
                                  locations = locations3.4)
denver.model.3.4
```

### Estimate model parameters 

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.3.4, all=FALSE)
names

# x.init.3.4 <- cbind(c(0, 0, 0, 0, 0),
#                     c(-1, 0, -1, -1, 0, -1, -1),
#                     c(-1, 0, -1, -5, 0, -5, -1),
#                     c(-5, 0, -5, -1, 0, -1, -5),
#                     c(-5, 0, -5, -5, 0, -5, -5),
#                     c(-1, 2, -1, -1, 2, -1, -1),
#                     c(-1, 2, -1, -5, 2, -5, -1),
#                     c(-5, 2, -5, -1, 2, -1, -5),
#                     c(-5, 2, -5, -5, 2, -5, -5),
#                     c(-1, 4, -1, -1, 4, -1, -1),
#                     c(-1, 4, -1, -5, 4, -5, -1),
#                     c(-5, 4, -5, -5, 4, -1, -5),
#                     c(-5, 4, -5, -5, 4, -5, -5))
# x.init.3.4[nrow(x.init.3.4),] <- 0

x.init.3.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0),
                    c(-5, 4, -5, -1, 8, -1, -5),
                    c(-5, 4, -5, -5, 8, -5, -5),
                    c(-10, 4, -1, -1, 8, -1, -5),
                    c(-10, 4, -1, -5, 8, -5, -5),
                    c(-5, 4, -10, -1, 8, -1, -5),
                    c(-5, 4, -10, -5, 8, -5, -5),
                    c(-10, 4, -10, -1, 8, -1, -5),
                    c(-10, 4, -10, -5, 8, -5, -5))

rownames(x.init.3.4) <- loglikeSTnames(denver.model.3.4, all=FALSE)
x.init.3.4

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.3.4 <- estimate.STmodel(denver.model.3.4, x.init.3.4)
print(est.denver.model.3.4)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.3.4 <- createCV(denver.model.3.4, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.3.4 <- sapply(split(denver.model.3.4$obs$ID, Ind.cv.3.4), unique)
print(sapply(ID.cv.3.4, length))
table(Ind.cv.3.4)

I.col.3.4 <- apply(sapply(ID.cv.3.4,function(x) denver.model.3.4$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.3.4) <- denver.model.3.4$locations$ID
print(I.col.3.4)

par(mfrow=c(1,1))
plot(denver.model.3.4$locations$long,
     denver.model.3.4$locations$lat,
     pch=23+floor(I.col.3.4/max(I.col.3.4)+.5), bg=I.col.3.4,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.3.4.cv <- coef(est.denver.model.3.4, pars="cov")[,c("par","init")]
x.init.3.4.cv
```

Run the model with cross validation.
```{r}
est.denver.3.4.cv <- estimateCV(denver.model.3.4, x.init.3.4.cv, Ind.cv.3.4)
print(est.denver.3.4.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.3.4, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.3.4.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.1, denver.model.3.4, est.denver.model.3.4, est.denver.3.4.cv,
     file = here::here("Results", "Denver_ST_Model_3.4.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.3.4.cv <- predictCV(denver.model.3.4, est.denver.3.4.cv, LTA = T)
pred.3.4.cv.log <- predictCV(denver.model.3.4, est.denver.3.4.cv,
                             LTA = T, transform="unbiased")

names(pred.3.4.cv)
summary(pred.3.4.cv)
summary(pred.3.4.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod3.4.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.3.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 3.5: Smoothing for beta fields and error term

### Create the model object (exp covariance structure)
Now I'm using the ```exp``` covariance structure for ```cov.beta``` (beta0 and beta1) and ```cov.nu```. 

```{r}
names(denver.data2.1$covars)

LUR3.5<- list(covar_fun, covar_fun)

cov.beta3.5 <-  list(covf=c("exp", "exp"), nugget = c(TRUE, TRUE))
cov.nu3.5 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations3.5 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.3.5 <- createSTmodel(denver.data2.1, LUR = LUR3.5,
                                  ST = "bc_st_no2",
                                  cov.beta = cov.beta3.5, cov.nu = cov.nu3.5,
                                  locations = locations3.5)
denver.model.3.5
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.3.5, all=FALSE)
names

# x.init.3.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, 0, -1, -5, 0, -1, -5),
#                     c(0, -5, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -5, -5, 0, -1, -5, 0, -1, -5),
#                     c(2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, 2, -1, -5, 2, -1, -5),
#                     c(2, -5, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -5, -5, 2, -1, -5, 2, -1, -5),
#                     c(4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, 4, -1, -5, 4, -1, -5),
#                     c(4, -5, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, 6, -1, -5, 6, -1, -5),
#                     c(6, -5, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -5, -5, 6, -1, -5, 6, -1, -5)
#                     )

x.init.3.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(6, -10, -5, 4, -10, -5, 4, -1, -5),
                    c(6, -5, -1, 4, -10, -1, 4, -1, -1),
                    c(6, -5, -5, 4, -10, -5, 4, -1, -5),
                    c(8, -10, -10, 6, -10, -10, 10, -1, -1),
                    c(8, -10, -5, 6, -10, -5, 10, -1, -5),
                    c(8, -10, -10, 6, -10, -5, 10, -10, -5)
                    )

rownames(x.init.3.5) <- loglikeSTnames(denver.model.3.5, all=FALSE)
x.init.3.5

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.3.5 <- estimate.STmodel(denver.model.3.5, x.init.3.5)
print(est.denver.model.3.5)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs))
Ind.cv.3.5 <- createCV(denver.model.3.5, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.3.5 <- sapply(split(denver.model.3.5$obs$ID, Ind.cv.3.5), unique)
print(sapply(ID.cv.3.5, length))
table(Ind.cv.3.5)

I.col.3.5 <- apply(sapply(ID.cv.3.5,function(x) denver.model.3.5$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.3.5) <- denver.model.3.5$locations$ID
print(I.col.3.5)

par(mfrow=c(1,1))
plot(denver.model.3.5$locations$long,
     denver.model.3.5$locations$lat,
     pch=23+floor(I.col.3.5/max(I.col.3.5)+.5), bg=I.col.3.5,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.3.5.cv <- coef(est.denver.model.3.5, pars="cov")[,c("par","init")]
x.init.3.5.cv
```

Run the model with cross validation.
```{r}

est.denver.3.5.cv <- estimateCV(denver.model.3.5, x.init.3.5.cv, Ind.cv.3.5)
print(est.denver.3.5.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.3.5, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.3.5.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data2.1, denver.model.3.5, est.denver.model.3.5, est.denver.3.5.cv,
     file = here::here("Results", "Denver_ST_Model_3.5.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.3.5.cv <- predictCV(denver.model.3.5, est.denver.3.5.cv, LTA = T)
pred.3.5.cv.log <- predictCV(denver.model.3.5, est.denver.3.5.cv,
                             LTA = T, transform="unbiased")

names(pred.3.5.cv)
summary(pred.3.5.cv)
summary(pred.3.5.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,3.5,1), mgp=c(2,1,0))
plot(pred.3.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod3.5.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,3.5,1), mgp=c(2,1,0))
plot(pred.3.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.3.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```


# Updated models (NO2 + BC, two basis functions, binary smoke indicator)
The models above only use one ST predictor (NO2 estimated at each sampling location using IDW). In this next set of models, I am adding a binary indicator for smoke to see it fit improves models fit. 

The smoke indicator is based on the methodology in the BC-wildfires paper, which is a modification of the methods used in Brey and Fischer (2016).

## New data object with the additional ST covariate

```{r}
denver.data4 <- createSTdata(obs = bc_obs2,
                             covars = bc_sp_cov,
                             SpatioTemporal = list(bc_st_no2 = bc_st_no22,
                                                   bc_st_smk = bc_st_smk2))
D4 <- createDataMatrix(denver.data4)
denver.data4
```

Add the two basis functions:
```{r}
denver.data4.2 <- denver.data4
denver.data4.2$trend <- no2_bc_STdata2$trend
print(denver.data4.2)
```

## Model 4.1: simplest form 

### Create the model object 
For this version of the model, use ```iid``` for both ```cov.beta``` (beta0, beta1, beta2) and ```cov.nu``` (error term).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data4.2$covars)

LUR4.1 <- list(covar_fun, covar_fun, covar_fun)

cov.beta4.1 <-  list(covf="iid", nugget = T)
cov.nu4.1 <- list(covf="iid", nugget = T, random.effect = FALSE)
locations4.1 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.4.1 <- createSTmodel(denver.data4.2, LUR = LUR4.1,
                                  ST = c("bc_st_no2", "bc_st_smk"),
                                  cov.beta = cov.beta4.1, cov.nu = cov.nu4.1,
                                  locations = locations4.1)
denver.model.4.1
```

### Estimate model parameters

We want to try a number of initial conditions to make sure we find the "right" solution.

```{r}
names <- loglikeSTnames(denver.model.4.1, all=FALSE)
names

# x.init.4.1 <- cbind(c(rep(-1, 4)), c(rep(-2, 4)), c(rep(-3, 4)),
#                     c(rep(-4, 4)), c(rep(-5, 4)), c(rep(-6, 4)))
# x.init.4.1[nrow(x.init.4.1),] <- 0

x.init.4.1 <- cbind(c(0, 0, 0),
                    c(-5, -5, -5, -5),
                    c(-8, -5, -5, -5),
                    c(-8, -8, -5, -5),
                    c(-8, -8, -8, -8))

rownames(x.init.4.1) <- loglikeSTnames(denver.model.4.1, all=FALSE)
x.init.4.1

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.4.1 <- estimate.STmodel(denver.model.4.1, x.init.4.1)
print(est.denver.model.4.1)
```

### Cross-validation

Define the CV groups (and don't forget to set the seed here!)
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.4.1 <- createCV(denver.model.4.1, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.4.1 <- sapply(split(denver.model.4.1$obs$ID, Ind.cv.4.1), unique)
print(sapply(ID.cv.4.1, length))
table(Ind.cv.4.1)

I.col.4.1 <- apply(sapply(ID.cv.4.1,function(x) denver.model.4.1$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.4.1) <- denver.model.4.1$locations$ID
print(I.col.4.1)

par(mfrow=c(1,1))
plot(denver.model.4.1$locations$long,
     denver.model.4.1$locations$lat,
     pch=23+floor(I.col.4.1/max(I.col.4.1)+.5), bg=I.col.4.1,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.4.1.cv <- coef(est.denver.model.4.1, pars="cov")[,c("par","init")]
x.init.4.1.cv
```

Run the model with cross validation.
```{r}
est.denver.4.1.cv <- estimateCV(denver.model.4.1, x.init.4.1.cv, Ind.cv.4.1)
print(est.denver.4.1.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.4.1, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.4.1.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data4.2, denver.model.4.1, est.denver.model.4.1, est.denver.4.1.cv,
     file = here::here("Results", "Denver_ST_Model_4.1.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.4.1.cv <- predictCV(denver.model.4.1, est.denver.4.1.cv, LTA = T)
pred.4.1.cv.log <- predictCV(denver.model.4.1, est.denver.4.1.cv,
                             LTA = T, transform="unbiased")

names(pred.4.1.cv)
summary(pred.4.1.cv)
summary(pred.4.1.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod4.1.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.1.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.1.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 4.2: Smoothing in the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta, beta1, beta2) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data4.2$covars)

LUR4.2 <- list(covar_fun, covar_fun, covar_fun)

cov.beta4.2 <-  list(covf = c("iid", "iid", "iid"), nugget = T)
cov.nu4.2 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations4.2 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.4.2 <- createSTmodel(denver.data4.2, LUR = LUR4.2,
                                  ST = c("bc_st_no2", "bc_st_smk"),
                                  cov.beta = cov.beta4.2, cov.nu = cov.nu4.2,
                                  locations = locations4.2)
denver.model.4.2
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.4.2, all=FALSE)
names

# x.init.4.2 <- cbind(c(0, 0, 0, 0, 0, 0),
#                     c(-1, -1, -1, 0, -1, -1),
#                     c(-1, -1, -1, 0, -5, -1),
#                     c(-5, -5, -5, 0, -1, -5),
#                     c(-5, -5, -5, 0, -5, -5),
#                     c(-1, -1, -1, 2, -1, -1),
#                     c(-1, -1, -1, 2, -5, -1),
#                     c(-5, -5, -5, 2, -1, -5),
#                     c(-5, -5, -5, 2, -5, -5),
#                     c(-1, -1, -1, 4, -1, -1),
#                     c(-1, -1, -1, 4, -5, -1),
#                     c(-5, -5, -5, 4, -1, -5),
#                     c(-5, -5, -5, 4, -5, -5))

x.init.4.2 <- cbind(c(0, 0, 0, 0, 0, 0),
                    c(-10, -5, -5, 4, -3, -5),
                    c(-10, -5, -5, 4, -5, -5),
                    c(-10, -5, -5, 6, -3, -5),
                    c(-10, -5, -5, 6, -5, -5),
                    c(-10, -5, -5, 8, -3, -5),
                    c(-10, -5, -5, 8, -5, -5),
                    c(-10, -5, -5, 10, -3, -5),
                    c(-10, -5, -5, 10, -5, -5))

rownames(x.init.4.2) <- loglikeSTnames(denver.model.4.2, all=FALSE)
x.init.4.2

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.4.2 <- estimate.STmodel(denver.model.4.2, x.init.4.2)
print(est.denver.model.4.2)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.4.2 <- createCV(denver.model.4.2, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.4.2 <- sapply(split(denver.model.4.2$obs$ID, Ind.cv.4.2), unique)
print(sapply(ID.cv.4.2, length))
table(Ind.cv.4.2)

I.col.4.2 <- apply(sapply(ID.cv.4.2, function(x) denver.model.4.2$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.4.2) <- denver.model.4.2$locations$ID
print(I.col.4.2)

par(mfrow=c(1,1))
plot(denver.model.4.2$locations$long,
     denver.model.4.2$locations$lat,
     pch=23+floor(I.col.4.2/max(I.col.4.2)+.5), bg=I.col.4.2,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.4.2.cv <- coef(est.denver.model.4.2, pars="cov")[,c("par","init")]
x.init.4.2.cv
```

Run the model with cross validation
```{r}

est.denver.4.2.cv <- estimateCV(denver.model.4.2, x.init.4.2.cv, Ind.cv.4.2)
print(est.denver.4.2.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.4.2, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.4.2.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data4.2, denver.model.4.2, est.denver.model.4.2, est.denver.4.2.cv,
     file = here::here("Results", "Denver_ST_Model_4.2.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.4.2.cv <- predictCV(denver.model.4.2, est.denver.4.2.cv, LTA = T)
pred.4.2.cv.log <- predictCV(denver.model.4.2, est.denver.4.2.cv,
                             LTA = T, transform="unbiased")

names(pred.4.2.cv)
summary(pred.4.2.cv)
summary(pred.4.2.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod4.2.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.2.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.2.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```


## Model 4.3: Smoothing in beta0 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta1, beta2) and ```exp``` for ```cov.beta``` (beta0) and  ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
names(denver.data4.2$covars)

LUR4.3 <- list(covar_fun, covar_fun, covar_fun)

cov.beta4.3 <-  list(covf = c("exp", "iid", "iid"), nugget = c(T, T, T))
cov.nu4.3 <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations4.3 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.4.3 <- createSTmodel(denver.data4.2, LUR = LUR4.3,
                                  ST = c("bc_st_no2", "bc_st_smk"),
                                  cov.beta = cov.beta4.3, cov.nu = cov.nu4.3,
                                  locations = locations4.3)
denver.model.4.3
```

### Estimate model parameters

Josh gave some guidance on how to set up the initial values (NOTE: the ```iid``` model doesn't have range or sill values, but the ```exp``` model does):

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.4.3, all=FALSE)
names

# x.init.4.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, -5, -5, 0, -1, -1),
#                     c(0, -5, -1, -1, -1, 0, -5, -5),
#                     c(0, -5, -5, -5, -5, 0, -5, -5),
#                     c(2, -1, -1, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, -5, -5, 2, -1, -1),
#                     c(2, -5, -1, -1, -1, 2, -5, -5),
#                     c(2, -5, -5, -5, -5, 2, -5, -5),
#                     c(4, -1, -1, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, -5, -5, 4, -1, -1),
#                     c(4, -5, -1, -1, -1, 4, -5, -5),
#                     c(4, -5, -5, -5, -5, 4, -5, -5),
#                     c(6, -1, -1, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, -5, -5, 6, -1, -1),
#                     c(6, -5, -1, -1, -1, 6, -5, -5),
#                     c(6, -5, -5, -5, -5, 6, -5, -5)
#                     )
# x.init.4.3[nrow(x.init.4.3),] <- 0

x.init.4.3 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0),
                    c(6, -5, -5, -5, -5, 4, -3, -5),
                    c(6, -5, -5, -5, -5, 6, -5, -5),
                    c(6, -5, -5, -5, -5, 8, -3, -5),
                    c(6, -5, -5, -5, -5, 10, -5, -5),
                    c(6, -10, -10, -5, -5, 4, -3, -5),
                    c(6, -10, -10, -5, -5, 6, -5, -5),
                    c(6, -10, -10, -5, -5, 8, -3, -5),
                    c(6, -10, -10, -5, -5, 10, -5, -5)
                    )

rownames(x.init.4.3) <- loglikeSTnames(denver.model.4.3, all=FALSE)
x.init.4.3

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.4.3 <- estimate.STmodel(denver.model.4.3, x.init.4.3)
print(est.denver.model.4.3)
```

### Cross-validation

Define the CV groups
```{r}

set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.4.3 <- createCV(denver.model.4.3, groups = 10, #min.dist = .1,
                           subset = paste0("d_", c(1:60)))

ID.cv.4.3 <- sapply(split(denver.model.4.3$obs$ID, Ind.cv.4.3), unique)
print(sapply(ID.cv.4.3, length))
table(Ind.cv.4.3)

I.col.4.3 <- apply(sapply(ID.cv.4.3, function(x) denver.model.4.3$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.4.3) <- denver.model.4.3$locations$ID
print(I.col.4.3)

par(mfrow=c(1,1))
plot(denver.model.4.3$locations$long,
     denver.model.4.3$locations$lat,
     pch=23+floor(I.col.4.3/max(I.col.4.3)+.5), bg=I.col.4.3,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.4.3.cv <- coef(est.denver.model.4.3, pars="cov")[,c("par","init")]
x.init.4.3.cv
```

Run the model with cross validation
```{r}

est.denver.4.3.cv <- estimateCV(denver.model.4.3, x.init.4.3.cv, Ind.cv.4.3)
print(est.denver.4.3.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.4.3, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.4.3.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data4.2, denver.model.4.3, est.denver.model.4.3, est.denver.4.3.cv,
     file = here::here("Results", "Denver_ST_Model_4.3.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.4.3.cv <- predictCV(denver.model.4.3, est.denver.4.3.cv, LTA = T)
pred.4.3.cv.log <- predictCV(denver.model.4.3, est.denver.4.3.cv,
                             LTA = T, transform="unbiased")

names(pred.4.3.cv)
summary(pred.4.3.cv)
summary(pred.4.3.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod4.3.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.3.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.3.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```

## Model 4.4: Smoothing for beta1, beta2 and the error term

### Create the model object 
For this version of the model, use ```iid``` for ```cov.beta``` (beta0) and ```exp``` for ```cov.beta``` (beta1, beta2) ```cov.nu``` (error).

```{r}
names(denver.data4.2$covars)

LUR4.4 <- list(covar_fun, covar_fun, covar_fun)

cov.beta4.4 <-  list(covf=c("iid", "exp", "exp"), nugget = c(TRUE, TRUE, TRUE))
cov.nu4.4 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations4.4 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.4.4 <- createSTmodel(denver.data4.2, LUR = LUR4.4,
                                  ST = c("bc_st_no2", "bc_st_smk"),
                                  cov.beta = cov.beta4.4, cov.nu = cov.nu4.4,
                                  locations = locations4.4)
denver.model.4.4
```

### Estimate model parameters 

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.4.4, all=FALSE)
names
# 
# x.init.4.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(-1, 0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(-1, 0, -1, -5, 0, -5, -1, 0, -5, -1),
#                     c(-5, 0, -5, -1, 0, -1, -5, 0, -1, -5),
#                     c(-5, 0, -5, -5, 0, -5, -5, 0, -5, -5),
#                     c(-1, 2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(-1, 2, -1, -5, 2, -5, -1, 2, -5, -1),
#                     c(-5, 2, -5, -1, 2, -1, -5, 2, -1, -5),
#                     c(-5, 2, -5, -5, 2, -5, -5, 2, -5, -5),
#                     c(-1, 4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(-1, 4, -1, -5, 4, -5, -1, 4, -5, -1),
#                     c(-5, 4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(-5, 4, -5, -5, 4, -5, -5, 4, -5, -5),
#                     c(-1, 6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(-1, 6, -1, -5, 6, -5, -1, 6, -5, -1),
#                     c(-5, 6, -5, -5, 6, -1, -5, 6, -1, -5),
#                     c(-5, 6, -5, -5, 6, -5, -5, 6, -5, -5))
# x.init.4.4[nrow(x.init.4.4),] <- 0

x.init.4.4 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(-5, 4, -5, -5, 6, -5, -5, 6, -5, -5),
                    c(-5, 4, -5, -5, 6, -5, -5, 6, -5, -5),
                    c(-10, 4, -10, -1, 6, -5, -10, 10, -5, -5),
                    c(-10, 4, -10, -5, 6, -5, -10, 10 -5, -5),
                    c(-13, 4, -10, -1, 6, -5, -13, 10, -5, -5),
                    c(-13, 4, -10, -5, 6, -5, -13, 10, -5, -5))

rownames(x.init.4.4) <- loglikeSTnames(denver.model.4.4, all=FALSE)
x.init.4.4

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.4.4 <- estimate.STmodel(denver.model.4.4, x.init.4.4)
print(est.denver.model.4.4)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.4.4 <- createCV(denver.model.4.4, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.4.4 <- sapply(split(denver.model.4.4$obs$ID, Ind.cv.4.4), unique)
print(sapply(ID.cv.4.4, length))
table(Ind.cv.4.4)

I.col.4.4 <- apply(sapply(ID.cv.4.4,function(x) denver.model.4.4$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.4.4) <- denver.model.4.4$locations$ID
print(I.col.4.4)

par(mfrow=c(1,1))
plot(denver.model.4.4$locations$long,
     denver.model.4.4$locations$lat,
     pch=23+floor(I.col.4.4/max(I.col.4.4)+.5), bg=I.col.4.4,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.4.4.cv <- coef(est.denver.model.4.4, pars="cov")[,c("par","init")]
x.init.4.4.cv
```

Run the model with cross validation.
```{r}
est.denver.4.4.cv <- estimateCV(denver.model.4.4, x.init.4.4.cv, Ind.cv.4.4)
print(est.denver.4.4.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.4.4, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
            col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.4.4.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data4.2, denver.model.4.4, est.denver.model.4.4, est.denver.4.4.cv,
     file = here::here("Results", "Denver_ST_Model_4.4.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.4.4.cv <- predictCV(denver.model.4.4, est.denver.4.4.cv, LTA = T)
pred.4.4.cv.log <- predictCV(denver.model.4.4, est.denver.4.4.cv,
                             LTA = T, transform="unbiased")

names(pred.4.4.cv)
summary(pred.4.4.cv)
summary(pred.4.4.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod4.4.jpeg"),
     width = 8, height = 4, units = "in", res = 500)
par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.4.4.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.4.4.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")
dev.off()
```



## Model 4.5: Smoothing for beta fields and error term

### Create the model object (exp covariance structure)

Now I'm using the ```exp``` covariance structure for ```cov.beta``` and ```cov.nu```.

```{r}
names(denver.data4.2$covars)

LUR4.5<- list(covar_fun, covar_fun, covar_fun)

cov.beta4.5 <-  list(covf=c("exp", "exp", "exp"), nugget = c(TRUE, TRUE, TRUE))
cov.nu4.5 <- list(covf="exp", nugget = T, random.effect = FALSE)
locations4.5 <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.4.5 <- createSTmodel(denver.data4.2, LUR = LUR4.5,
                                  ST = c("bc_st_no2", "bc_st_smk"),
                                  cov.beta = cov.beta4.5, cov.nu = cov.nu4.5,
                                  locations = locations4.5)
denver.model.4.5
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}

names <- loglikeSTnames(denver.model.4.5, all=FALSE)
names

# x.init.4.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
#                     c(0, -1, -1, 0, -1, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -1, -5, 0, -1, -5, 0, -1, -5, 0, -1, -5),
#                     c(0, -5, -1, 0, -5, -1, 0, -1, -1, 0, -1, -1),
#                     c(0, -5, -5, 0, -5, -5, 0, -1, -5, 0, -1, -5),
#                     c(2, -1, -1, 2, -1, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -1, -5, 2, -1, -5, 2, -1, -5, 2, -1, -5),
#                     c(2, -5, -1, 2, -5, -1, 2, -1, -1, 2, -1, -1),
#                     c(2, -5, -5, 2, -5, -5, 2, -1, -5, 2, -1, -5),
#                     c(4, -1, -1, 4, -1, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -1, -5, 4, -1, -5, 4, -1, -5, 4, -1, -5),
#                     c(4, -5, -1, 4, -5, -1, 4, -1, -1, 4, -1, -1),
#                     c(4, -5, -5, 4, -5, -5, 4, -1, -5, 4, -1, -5),
#                     c(6, -1, -1, 6, -1, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -1, -5, 6, -1, -5, 6, -1, -5, 6, -1, -5),
#                     c(6, -5, -1, 6, -5, -1, 6, -1, -1, 6, -1, -1),
#                     c(6, -5, -5, 6, -5, -5, 6, -1, -5, 6, -1, -5)
#                     )

x.init.4.5 <- cbind(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0),
                    c(6, -5, -1, 6, -5, -1, 6, -1, -1, 6, -1, -1),
                    c(6, -5, -5, 6, -5, -5, 6, -5, -10, 10, -5, -5),
                    c(6, -10, -10, 6, -10, -10, 6, -10, -10, 6, -10, -1),
                    c(6, -10, -5, 6, -10, -5, 6, -5, -5, 6, -5, -5))

rownames(x.init.4.5) <- loglikeSTnames(denver.model.4.5, all=FALSE)
x.init.4.5

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.4.5 <- estimate.STmodel(denver.model.4.5, x.init.4.5)
print(est.denver.model.4.5)
```

### Cross-validation

Define the CV groups 
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.4.5 <- createCV(denver.model.4.5, groups = 10, #min.dist = .1,
                       subset = paste0("d_", c(1:60)))

ID.cv.4.5 <- sapply(split(denver.model.4.5$obs$ID, Ind.cv.4.5), unique)
print(sapply(ID.cv.4.5, length))
table(Ind.cv.4.5)

I.col.4.5 <- apply(sapply(ID.cv.4.5,function(x) denver.model.4.5$locations$ID%in% x), 1,
                   function(x) if(sum(x)==1) which(x) else 0)
names(I.col.4.5) <- denver.model.4.5$locations$ID
print(I.col.4.5)

par(mfrow=c(1,1))
plot(denver.model.4.5$locations$long,
     denver.model.4.5$locations$lat,
     pch=23+floor(I.col.4.5/max(I.col.4.5)+.5), bg=I.col.4.5,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.4.5.cv <- coef(est.denver.model.4.5, pars="cov")[,c("par","init")]
x.init.4.5.cv
```

Run the model with cross validation.

When I ran the CV model I got the following error for the 5th CV set

Error in solve.default(res[[i]]$hessian) : 
  system is computationally singular: reciprocal condition number = 3.3547e-17
```{r}

# est.denver.4.5.cv <- estimateCV(denver.model.4.5, x.init.4.5.cv, Ind.cv.4.5)
# print(est.denver.4.5.cv)
# 
# par(mfrow=c(1,1), mar=c(13.5,4.5,.5,.5), las=2)
# with(coef(est.denver.model.4.5, pars="all"),
#      plotCI((1:length(par))+.3, par, uiw=1.96*sd,
#             col=2, xlab="", xaxt="n", ylab=""))
# boxplot(est.denver.4.5.cv, "all", boxwex=.4, col="grey", add=TRUE)
# 
# #' Save the results as an .rdata object
# save(denver.data4.2, denver.model.4.5, est.denver.model.4.5, est.denver.4.5.cv,
#      file = here::here("Results", "Denver_ST_Model_4.5.rdata"))
```

### Prediction using the CV model
<!-- Making predictions using the CV model. Printing out the CV summary statistics as well -->

I didn't make predictions with the CV model since it wasn't successfully implemented

```{r}
# pred.4.5.cv <- predictCV(denver.model.4.5, est.denver.4.5.cv, LTA = T)
# pred.4.5.cv.log <- predictCV(denver.model.4.5, est.denver.4.5.cv,
#                              LTA = T, transform="unbiased")
# 
# names(pred.4.5.cv)
# summary(pred.4.5.cv)
# summary(pred.4.5.cv.log)
# 
# par(mfrow=c(1,2), mar=c(3.3,3.3,4.5,1), mgp=c(2,1,0))
# plot(pred.4.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.4.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# 
# jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_Mod4.5.jpeg"),
#      width = 8, height = 4, units = "in", res = 500)
# par(mfrow=c(1,2), mar=c(3.3,3.3,4.5,1), mgp=c(2,1,0))
# plot(pred.4.5.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.4.5.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# dev.off()
```

# Summary Table

```{r, include = F, echo = F}

sum_tab <- data.frame(model = c("Model 1.1", "Model 1.2", "Model 1.3", "Model 1.4", "Model 1.5",
                                "Model 2.1", "Model 2.2", "Model 2.3", "Model 2.4", "Model 2.5",
                                "Model 3.1", "Model 3.2", "Model 3.3", "Model 3.4", "Model 3.5",
                                "Model 4.1", "Model 4.2", "Model 4.3", "Model 4.4", "Model 4.5"),
                      trend_data = c(rep("BC", 5), rep("NO2 + BC", 15)),
                      trend_start = c(rep(2016, 5), rep(2009, 15)),
                      trend_end = rep(2019, 20),
                      ST_vars = c(rep("bc_st_no2", 5), rep("bc_st_no22", 10), rep("bc_st_no22, bc_st_smk2", 5)),
                      cov_beta0 = rep(c("iid", "iid", "exp", "iid", "exp"), 4),
                      cov_beta1 = rep(c("iid", "iid", "iid", "exp", "exp"), 4),
                      cov_beta2 = rep(c(NA, NA, NA, NA, NA, "iid", "iid", "iid", "exp", "exp"), 2),
                      cov_nu_error = rep(c("iid", "exp", "exp", "exp", "exp"), 4),
                      no_basis_fns = rep(c(rep(1, 5), rep(2, 5)), 2),
                      df_per_year = rep(4, 20),
                      all_converge = c(ifelse(exists("est.denver.1.1.cv"),all(est.denver.1.1.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.1.2.cv"),all(est.denver.1.2.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.1.3.cv"),all(est.denver.1.3.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.1.4.cv"),all(est.denver.1.4.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.1.5.cv"),all(est.denver.1.5.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.2.1.cv"),all(est.denver.2.1.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.2.2.cv"),all(est.denver.2.2.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.2.3.cv"),all(est.denver.2.3.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.2.4.cv"),all(est.denver.2.4.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.2.5.cv"),all(est.denver.2.5.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.3.1.cv"),all(est.denver.3.1.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.3.2.cv"),all(est.denver.3.2.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.3.3.cv"),all(est.denver.3.3.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.3.4.cv"),all(est.denver.3.4.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.3.5.cv"),all(est.denver.3.5.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.4.1.cv"),all(est.denver.4.1.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.4.2.cv"),all(est.denver.4.2.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.4.3.cv"),all(est.denver.4.3.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.4.4.cv"),all(est.denver.4.4.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.4.5.cv"),all(est.denver.4.5.cv$status$convergence), NA)),
                      all_conv = c(ifelse(exists("est.denver.1.1.cv"),all(est.denver.1.1.cv$status$conv), NA),
                                       ifelse(exists("est.denver.1.2.cv"),all(est.denver.1.2.cv$status$conv), NA),
                                       ifelse(exists("est.denver.1.3.cv"),all(est.denver.1.3.cv$status$conv), NA),
                                       ifelse(exists("est.denver.1.4.cv"),all(est.denver.1.4.cv$status$conv), NA),
                                       ifelse(exists("est.denver.1.5.cv"),all(est.denver.1.5.cv$status$conv), NA),
                                       ifelse(exists("est.denver.2.1.cv"),all(est.denver.2.1.cv$status$conv), NA),
                                       ifelse(exists("est.denver.2.2.cv"),all(est.denver.2.2.cv$status$conv), NA),
                                       ifelse(exists("est.denver.2.3.cv"),all(est.denver.2.3.cv$status$conv), NA),
                                       ifelse(exists("est.denver.2.4.cv"),all(est.denver.2.4.cv$status$conv), NA),
                                       ifelse(exists("est.denver.2.5.cv"),all(est.denver.2.5.cv$status$conv), NA),
                                       ifelse(exists("est.denver.3.1.cv"),all(est.denver.3.1.cv$status$conv), NA),
                                       ifelse(exists("est.denver.3.2.cv"),all(est.denver.3.2.cv$status$conv), NA),
                                       ifelse(exists("est.denver.3.3.cv"),all(est.denver.3.3.cv$status$conv), NA),
                                       ifelse(exists("est.denver.3.4.cv"),all(est.denver.3.4.cv$status$conv), NA),
                                       ifelse(exists("est.denver.3.5.cv"),all(est.denver.3.5.cv$status$conv), NA),
                                       ifelse(exists("est.denver.4.1.cv"),all(est.denver.4.1.cv$status$conv), NA),
                                       ifelse(exists("est.denver.4.2.cv"),all(est.denver.4.2.cv$status$conv), NA),
                                       ifelse(exists("est.denver.4.3.cv"),all(est.denver.4.3.cv$status$conv), NA),
                                       ifelse(exists("est.denver.4.4.cv"),all(est.denver.4.4.cv$status$conv), NA),
                                       ifelse(exists("est.denver.4.5.cv"),all(est.denver.4.5.cv$status$conv), NA)),
                      cv_rmse_obs = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$RMSE[1,3], 2), NA)),
                      cv_rmse_avg = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$RMSE[2,3], 2), NA)),
                      cv_r2_obs = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$R2[1,3], 2), NA)),
                      cv_r2_avg = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$R2[2,3], 2), NA)),
                      cv_coverage_obs = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$coverage[1,1], 2), NA)),
                      cv_coverage_avg = c(ifelse(exists("pred.1.1.cv.log"), round(summary(pred.1.1.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.1.2.cv.log"), round(summary(pred.1.2.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.1.3.cv.log"), round(summary(pred.1.3.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.1.4.cv.log"), round(summary(pred.1.4.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.1.5.cv.log"), round(summary(pred.1.5.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.2.1.cv.log"), round(summary(pred.2.1.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.2.2.cv.log"), round(summary(pred.2.2.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.2.3.cv.log"), round(summary(pred.2.3.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.2.4.cv.log"), round(summary(pred.2.4.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.2.5.cv.log"), round(summary(pred.2.5.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.3.1.cv.log"), round(summary(pred.3.1.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.3.2.cv.log"), round(summary(pred.3.2.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.3.3.cv.log"), round(summary(pred.3.3.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.3.4.cv.log"), round(summary(pred.3.4.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.3.5.cv.log"), round(summary(pred.3.5.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.4.1.cv.log"), round(summary(pred.4.1.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.4.2.cv.log"), round(summary(pred.4.2.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.4.3.cv.log"), round(summary(pred.4.3.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.4.4.cv.log"), round(summary(pred.4.4.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.4.5.cv.log"), round(summary(pred.4.5.cv.log)$coverage[2,1], 2), NA))
)

```

```{r, echo = F, results = "asis"}
library(knitr)
kable(sum_tab, caption = "Summary of model diagnostics") %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


# Additional Notes

## All of Josh Keller's advice for fitting the intital models
Pasting here for safe keeping

Your sleuthing about that error is correct. A singular hessian matrix (the
hessian is a like a matrix analogue of a second derivative) is symptomatic of
the optimization algorithm not finding the true minimum.  I remember this error
all too well! To deal with this, I suggest the following:

- Use the version of estimate.STmodel() in the attached file instead of
the one that is in the package. If you load the package, then source this file, it will overwrite the package function so you can use this version. This version has two changes:

    - It changes the error handling to catch the non-invertible warning
    and output an obviously wrong value for the standard deviation (-1000). This doesnt fix the problem at all, but keeps the function from aborting, as the package version does, since the abort makes you lose all of your results that far. This makes it more practical to use multiple starting values in a single function call, since it wont abort half way through.
    - It uses the function hessian() from the `numDeriv` package to provide a more numerically-accurate estimate of the Hessian matrix. The package version of the hessian is still stored in the `optim_hessian` object, while the new version is in the `hessian` object.  This means youll need to install the numDeriv package.

- Try a set of different initial conditions. The numerical algorithm used
to maximize the likelihood can get stuck in some areas, so using a set of several initial conditions is a good way to try to ensure that youre getting the best solution. The package should be able to loop through several initial values and pick the one giving the best result. Let me know if youd like any suggested values to try.

- Try a simpler model. Im not sure exactly what model structure youre using, but it often helps to start with a smaller model (e.g. 1 time trend, and iid covariance structure) to get a sense of the ranges of the parameters. You can then use the estimates from the simpler model as approximate starting values for a more complicated fit.

