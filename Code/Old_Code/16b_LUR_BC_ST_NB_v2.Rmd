---
title: "Fitting the ST model for the LUR"
author: "Sheena Martenies"
date: "07/20/2020"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings = F)
```

# Introduction

This version of the model builds on the analysis presented in `16b_LUR_BC_ST_NB`. There are some key changes to the model, described below.

When examining the BC predictions for 2009-2019, I noticed an issue with the predictions for the winter of 09/10. Basically, we were predicting concentrations that were twice as high as they should have been. We didn't really notice them before because the model was performing well for more recent years. I met with Josh on 7/15/2020 to discuss. There are a few problems that that might be contributing to this issue:

- We only have a handful of NO2 monitors in general, and only one goes back to 2009
- One monitor was started in 2010, and when looking at the time series, the smoothed function shows a sharp decrease at the beginning of the year.

Josh and I discussed some potenital strategies to improve model fit for the 2009-2010 period. These initial steps include:

- adding PM2.5 back in with the NO2 and BC data
- only including the four PM2.5 monitors with long-term trends (080050005, 080130003, 080130012, 080310002)
- testing out one vs. two basis functions 
- dropping NO2 monitor 080590006 from the analysis since it was installed so recently
- making sure to include NO2 at the BC monitoring site as well (make sure the names are distinct!)

Main take aways:

- It looks like these three models are performing comparably to the previous version that did not include PM2.5 in the trend data
- However, the 2-basis function model (Model B) still has some residual autocorrelation in at the central site
- This autocorrelation goes away when we use 3 trend functions, but then the predictions for the 2009-2019 period are different from what we expect

**Update 07/20/2020**
After some prelimiary models, Josh offered some additional feedback:

- The model using three basis functions likely doesn't have enough data to support it (which is good, because it wasn't giving us believable results anyway)
- The trends in PM and NO2 might be influenced by some more extreme values, so I'm going to try log-transforming them first and then scaling the variables

**Main take aways (pt 2)**:

- After talking with Josh, we decided to use Model B here. T

```{r, echo = F}
# library(devtools)
# install_version("SpatioTemporal", version = "1.1.9.1", repos = "https://cran.r-project.org")

library(SpatioTemporal)
library(numDeriv)
library(Matrix)
library(plotrix)
library(maps)
```

```{r, echo = F, include = F, warnings = F}
library(sf)
library(gstat)
library(sp)
library(raster)
library(ggplot2)
library(ggmap)
library(ggsn)
library(ggthemes)
library(extrafont)
library(GGally)
library(ggcorrplot)
library(stringr)
library(tidyverse)
library(lubridate)
library(readxl)
library(viridis)
library(caret)
library(knitr)
library(kableExtra)

#' For ggplots
# loadfonts(device = "win")

simple_theme <- theme(
  #aspect.ratio = 1,
  text  = element_text(size = 12, color = 'black'),
  panel.spacing.y = unit(0,"cm"),
  panel.spacing.x = unit(0.25, "lines"),
  panel.grid.minor = element_line(color = "grey90"),
  panel.grid.major = element_line(color = "grey90"),
  panel.border=element_rect(fill = NA),
  panel.background=element_blank(),
  axis.ticks = element_line(colour = "black"),
  axis.text = element_text(color = "black", size=10),
  # legend.position = c(0.1,0.1),
  plot.margin=grid::unit(c(0,0,0,0), "mm"),
  legend.key = element_blank()
)

options(scipen = 9999) #avoid scientific notation

albers <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
ll_nad83 <- "+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0"
ll_wgs84 <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
```

# Fit time trends
Because of the issue with predicting BC for 2009-2010, we need to make some changes to how the time trends are estimated. Here I'm going to add PM2.5 from four monitors with long term trends. I'm also going to drop one NO2 monitor that isn't contributing much and I'm going to make sure that the BC and NO2 data at the near-road site are both included.

## Denver data set
Here I'm reading in the filter data sets and getting them into the right shape for the ```SpatioTemporal``` package. Here the ST covariates cover the entire study period (2009 - 2019).

```{r}

data_name <- "Combined_Filter_Data_AEA.csv"
lur_data <- read_csv(here::here("Data", data_name)) %>%
  filter(!is.na(lon)) %>%
  filter(indoor == 0) %>%
  #' QA filters
  filter(bc_ug_m3_dem > 0) %>% 
  filter(is.na(below_lod) | below_lod == 0) %>% 
  filter(is.na(low_volume_flag) | low_volume_flag == 0) %>% 
  filter(is.na(flow_rate_flag) | flow_rate_flag == 0) %>% 
  filter(is.na(is_blank) | is_blank == 0) %>% 
  filter(is.na(negative_pm_mass) | negative_pm_mass == 0) %>% 
  filter(is.na(potential_contamination) | potential_contamination == 0) %>% 
  #' Using Ben's LOD, but not imputing values
  filter(is.na(bc_mass_ug_corrected) | bc_mass_ug_corrected >= 1.41) %>% 
  #' Just campaigns 1-3 and the monitoring site
  filter(campaign %in% paste0("Campaign", c(1, 2, 3, "X")))

lur_data <- lur_data %>%
  rename("bc_ug_m3_raw" = "bc_ug_m3") %>%
  rename("bc_ug_m3" = "bc_ug_m3_dem")

camp_data <- filter(lur_data, campaign %in% paste0("Campaign", c(1, 2, 3)))
nrow(camp_data)
summary(camp_data$bc_ug_m3)
quantile(camp_data$bc_ug_m3, probs = c(0.05, 0.95))
sd(camp_data$bc_ug_m3)

site_data <- filter(lur_data, campaign == "CampaignX")
nrow(site_data)
summary(site_data$bc_ug_m3)
quantile(site_data$bc_ug_m3, probs = c(0.05, 0.95))
sd(site_data$bc_ug_m3)

#' Format the date variable to be Year-WeekNo. (starts on Sunday)
#' Convert that back to the first day of the week
lur_data <- lur_data %>%
  mutate(sample_week_no = format(StartDateLocal, "%Y-%W")) %>%
  mutate(sample_week = as.Date(cut(as.Date(StartDateLocal), "week")))

#' Add a unique site ID
ids <- read_csv(here::here("Data", "ST_Model_Site_IDs.csv"))

lur_data <- mutate(lur_data, site_id_lonlat = paste(lon, lat, sep = "_"))
lur_data <- left_join(lur_data, ids, by = "site_id_lonlat")

lur_data$site_id <- ifelse(lur_data$filter_id == "080310027", "central", lur_data$site_id)

#' Create an ID object for the central site
cent_id <- filter(lur_data, filter_id == "080310027") %>% 
  dplyr::select(site_id_lonlat) %>% 
  distinct()

#' Drop old ST covariates
#' Keep linking variables (sample_week and site_id_lonlat)
lur_data <- dplyr::select(lur_data, -c(StartDateLocal:units_smoke))

#' Get the new ST covariates
st_covariates_file_name <- "ST_Covariates_Filters_09_19_AEA.csv"
st_covariates <- read_csv(here::here("Data", st_covariates_file_name)) %>% 
  rename("sample_week" = "week") %>% 
  filter(year(sample_week) < 2019) %>% 
  left_join(ids, by = "site_id_lonlat") %>% 
  filter(site_id_lonlat %in% lur_data$site_id_lonlat)

#' Duplicate st covariates for the central site monitor
cent_st_covariates <- filter(st_covariates, site_id_lonlat == cent_id$site_id_lonlat) %>% 
  mutate(site_id = "central")

st_covariates <- bind_rows(st_covariates, cent_st_covariates)

st_cov_dates <- dplyr::select(st_covariates, site_id, sample_week) %>% 
  distinct()

#' Log-transformed BC observations with NA values for the missing dates
bc_obs <- lur_data %>%
  dplyr::select(site_id, sample_week, bc_ug_m3) %>%
  mutate(log_bc = log(bc_ug_m3)) %>%
  dplyr::select(-bc_ug_m3) 

#' Fill in the ST dates with NAs
bc_obs2 <- left_join(st_cov_dates, bc_obs, by = c("sample_week", "site_id"))

#' Pivot to a wide data frame
bc_obs2 <- bc_obs2 %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = log_bc) %>%
              # names_from = site_id, values_from = bc_ug_m3) %>%
  as.data.frame() %>%
  arrange(sample_week)
rownames(bc_obs2) <- bc_obs2$sample_week
bc_obs2$sample_week <- NULL
bc_obs2 <- as.matrix(bc_obs2)

bc_weeks <- rownames(bc_obs2)

#' Spatial covariates (scaled)
load(here::here("Results", "BC_LASSO_Model_3C.Rdata"))

lasso_coef_df <- data.frame(name = log_bc_lasso_coef3@Dimnames[[1]][log_bc_lasso_coef3@i + 1],
                            coefficient = log_bc_lasso_coef3@x)
covars <- as.character(lasso_coef_df$name[-c(1:2)])
covars 

covar_fun <- paste("~", paste(covars, collapse = " + "))
covar_fun

bc_sp_cov <- dplyr::select(lur_data, site_id, lon, lat, all_of(covars)) %>%
  distinct() %>%
  mutate_at(.vars = vars(covars),
            scale)

bc_sp_cov <- bc_sp_cov %>%
  rename(ID = site_id) %>%
  mutate(lon2 = lon, lat2 = lat) %>%
  st_as_sf(coords = c("lon2", "lat2"), crs = ll_wgs84) %>%
  st_transform(crs = albers)
sp_coords <- do.call(rbind, st_geometry(bc_sp_cov)) %>% as_tibble()
names(sp_coords) <- c("x", "y")

bc_sp_cov <- bind_cols(bc_sp_cov, sp_coords) %>%
  st_set_geometry(NULL) %>%
  as.data.frame()

bc_sp_cov$type <- ifelse(bc_sp_cov$ID == "central", "central", "dist")
bc_sp_cov$type <- as.factor(bc_sp_cov$type)

cor(bc_sp_cov[,covars])

#' NO2 and smoke spatiotemporal predictors
#' NO2 at each site estimated using IDW
bc_st_no2 <- dplyr::select(st_covariates, site_id, sample_week, idw_no2) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = idw_no2) %>%
  as.data.frame()
rownames(bc_st_no2) <- bc_st_no2$sample_week
bc_st_no2$sample_week <- NULL
bc_st_no2 <- as.matrix(bc_st_no2)

no2_weeks <- rownames(bc_st_no2)
setdiff(bc_weeks, no2_weeks)

#' Smoke day indicator variable based on WFS paper method (see Brey and Fischer, 2016)
bc_st_smk <- dplyr::select(st_covariates, site_id, sample_week, area_smoke_2sd) %>%
  pivot_wider(id_cols = sample_week,
              names_from = site_id, values_from = area_smoke_2sd) %>%
  as.data.frame()
rownames(bc_st_smk) <- bc_st_smk$sample_week
bc_st_smk$sample_week <- NULL
bc_st_smk <- as.matrix(bc_st_smk)

smk_weeks <- rownames(bc_st_smk)

setdiff(bc_weeks, smk_weeks)
```

Create a new ```denver.data``` object. 

```{r}
denver.data <- createSTdata(obs = bc_obs2,
                            covars = bc_sp_cov,
                            SpatioTemporal = list(bc_st_no2 = bc_st_no2,
                                                  bc_st_smk = bc_st_smk))
D <- createDataMatrix(denver.data)
denver.data
```

## Creating the NO2, BC, and PM data objects

Next we need to combine monitoring data for NO2, BC, and PM2.5 in order to calculate the time trends.

First up: NO2
```{r, include = F}

#' Denver Metro area counties
counties <- c("001", "005", "013", "014", "031", "059")

#' NO2 concentrations
no2_data <- read_csv(here::here("Data", "Monitor_NO2_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  filter(County_Code %in% counties) %>%
  arrange(Date_Local, monitor_id) %>%
  mutate(Arithmetic_Mean = ifelse(Arithmetic_Mean <= 0, NA, Arithmetic_Mean))

#' Drop monitor 080590006 because it was installed too recently and there isn't a long-term record
no2_data <- filter(no2_data, monitor_id != "080590006")
length(unique(no2_data$monitor_id))

#' Add an identifier to differentiate these measurements from collocated pollutants at the same site
no2_data$monitor_id <- paste0(no2_data$monitor_id, "_no2")
unique(no2_data$monitor_id)
```

Plot the time trends for NO2 at the monitors

**Original units (ppb)**
```{r}
no2_ts <- ggplot(data = no2_data, aes(x = Date_Local, y = Arithmetic_Mean)) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
no2_ts
```

**Log-transformed NO2**
```{r}
no2_ts2 <- ggplot(data = no2_data, aes(x = Date_Local, y = log(Arithmetic_Mean))) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
no2_ts2
```

**Log-transformed NO2** with a higher value for k in the smoothed term
```{r}
no2_ts3 <- ggplot(data = no2_data, aes(x = Date_Local, y = log(Arithmetic_Mean))) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam", formula = y ~ s(x, k = 30)) +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
no2_ts3
```

Organize the observations and spatial data. **Use the log-transformed NO2 here**
```{r}

#' NO2 observations
no2_obs <- no2_data %>% 
  dplyr::select(monitor_id, Date_Local, Arithmetic_Mean) %>%
  mutate(sample_week = as.character(as.Date(cut(as.Date(Date_Local), "week")))) %>%
  filter(sample_week %in% bc_weeks) %>% 
  group_by(monitor_id, sample_week) %>%
  mutate(log_mean = log(Arithmetic_Mean)) %>% 
  summarize(no2 = mean(log_mean, na.rm=T)) %>%
  pivot_wider(id_cols = sample_week,
              names_from = monitor_id, values_from = no2) %>%
  as.data.frame() %>%
  arrange(sample_week)

head(no2_obs)

# NO2 SP object
no2_data2 <- read_csv(here::here("Data", "Monitor_NO2_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(monitor_id = paste0(monitor_id, "_no2")) %>% 
  filter(monitor_id %in% colnames(no2_obs))

no2_coords <- do.call(rbind, st_geometry(no2_data2)) %>% as_tibble() 
names(no2_coords) <- c("x", "y")
no2_sp_lonlat <- no2_data2 %>%
  st_transform(crs = ll_wgs84)
no2_coords2 <- do.call(rbind, st_geometry(no2_sp_lonlat)) %>%
  as_tibble() %>% setNames(c("lon","lat"))

no2_sp_cov <- st_set_geometry(no2_data2, NULL) %>%
  dplyr::select(monitor_id) %>%
  rename(ID = monitor_id)
no2_sp_cov <- bind_cols(no2_sp_cov, no2_coords, no2_coords2) %>%
  as.data.frame() %>%
  distinct()
```

Next, BC at the central site monitor
```{r}

#' BC central site concentrations
bc_cent_data <- read_csv(here::here("Data", "Monitor_BC_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  filter(County_Code %in% counties) %>%
  arrange(Date_Local, monitor_id) %>% 
  mutate(Arithmetic_Mean = ifelse(Arithmetic_Mean <= 0.005, NA, Arithmetic_Mean))

#' Add an identifier to differentiate these measurements from collocated pollutants at the same site
bc_cent_data$monitor_id <- paste0(bc_cent_data$monitor_id, "_bc")
unique(bc_cent_data$monitor_id)
```

Plot the time trends for BC at the central site monitor

**Original units (ug/m3)** 
```{r}
bc_ts <- ggplot(data = bc_cent_data, aes(x = Date_Local, y = Arithmetic_Mean)) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  #facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
bc_ts
```

**Log-transformed BC** 
```{r}
bc_ts2 <- ggplot(data = bc_cent_data, aes(x = Date_Local, y = log(Arithmetic_Mean))) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  #facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
bc_ts2
```

Organize the observations and spatial data

**Use log-transformed data here**
```{r}
#' BC observations
bc_cent_obs <- bc_cent_data %>% 
  dplyr::select(monitor_id, Date_Local, Arithmetic_Mean) %>%
  mutate(sample_week = as.character(as.Date(cut(as.Date(Date_Local), "week")))) %>%
  filter(sample_week %in% bc_weeks) %>% 
  group_by(monitor_id, sample_week) %>%
  mutate(log_mean = log(Arithmetic_Mean)) %>% 
  summarize(bc = mean(log_mean, na.rm=T)) %>%
  pivot_wider(id_cols = sample_week,
              names_from = monitor_id, values_from = bc) %>%
  as.data.frame() %>%
  arrange(sample_week)

head(bc_cent_obs)

# BC Central Site SP object
bc_cent_data2 <- read_csv(here::here("Data", "Monitor_BC_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(monitor_id = paste0(monitor_id, "_bc")) %>% 
  filter(monitor_id %in% colnames(bc_cent_obs))

bc_cent_coords <- do.call(rbind, st_geometry(bc_cent_data2)) %>% as_tibble() 
names(bc_cent_coords) <- c("x", "y")
bc_cent_sp_lonlat <- bc_cent_data2 %>%
  st_transform(crs = ll_wgs84)
bc_cent_coords2 <- do.call(rbind, st_geometry(bc_cent_sp_lonlat)) %>%
  as_tibble() %>% setNames(c("lon","lat"))

bc_cent_sp_cov <- st_set_geometry(bc_cent_data2, NULL) %>%
  dplyr::select(monitor_id) %>%
  rename(ID = monitor_id)
bc_cent_sp_cov <- bind_cols(bc_cent_sp_cov, bc_cent_coords, bc_cent_coords2) %>%
  as.data.frame() %>%
  distinct()
```

Lastly, PM2.5 from select monitors with long-term records

```{r, include = F}

#' Denver Metro area counties
counties <- c("001", "005", "013", "014", "031", "059")

#' PM2.5 concentrations
pm_data <- read_csv(here::here("Data", "Monitor_PM_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  filter(County_Code %in% counties) %>%
  arrange(Date_Local, monitor_id)

#' Exclude monitors with really short-term records
pm_data <- filter(pm_data, monitor_id != '080310013')
length(unique(pm_data$monitor_id))

#' Add an identifier to differentiate these measurements from collocated pollutants at the same site
pm_data$monitor_id <- paste0(pm_data$monitor_id, "_pm")
unique(pm_data$monitor_id)

```

Plot the time trends for PM2.5 at the monitors

**Original units (ug/m3)**
```{r}
pm_ts <- ggplot(data = pm_data, aes(x = Date_Local, y = Arithmetic_Mean)) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
pm_ts
```

**Log-transformed PM2.5**
```{r}
pm_ts2 <- ggplot(data = pm_data, aes(x = Date_Local, y = log(Arithmetic_Mean))) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam") +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
pm_ts2
```

**Log-transformed PM2.5** with a higher value for k in the smoothed term
```{r}
pm_ts3 <- ggplot(data = pm_data, aes(x = Date_Local, y = log(Arithmetic_Mean))) +
  geom_point(aes(color = as.factor(monitor_id)), shape = 20, size = 1) +
  geom_smooth(se = F, color = "black", method = "gam", formula = y ~ s(x, k = 30)) +
  scale_color_viridis(name = "Monitor ID", discrete = T) +
  scale_x_date(date_breaks = "6 months", date_labels = "%m-%Y") +
  facet_wrap(. ~ monitor_id, scales = "fixed", ncol = 2) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  simple_theme
pm_ts3
```

Organize the observations and spatial data

**Use the log-transformed data here**
```{r}

#' PM2.5 observations
pm_obs <- pm_data %>% 
  dplyr::select(monitor_id, Date_Local, Arithmetic_Mean) %>%
  mutate(sample_week = as.character(as.Date(cut(as.Date(Date_Local), "week")))) %>%
  filter(sample_week %in% bc_weeks) %>% 
  group_by(monitor_id, sample_week) %>%
  mutate(log_mean = log(Arithmetic_Mean)) %>% 
  summarize(pm = mean(log_mean, na.rm=T)) %>%
  pivot_wider(id_cols = sample_week,
              names_from = monitor_id, values_from = pm) %>%
  as.data.frame() %>%
  arrange(sample_week)

head(pm_obs)

# PM SP object
pm_data2 <- read_csv(here::here("Data", "Monitor_PM_Data_AEA.csv")) %>%
  mutate(year = year(Date_Local)) %>% 
  filter(year %in% c(2009:2019)) %>% 
  dplyr::select(-year) %>% 
  st_as_sf(wkt = "WKT", crs = albers) %>% 
  mutate(monitor_id = paste0(monitor_id, "_pm")) %>% 
  filter(monitor_id %in% colnames(pm_obs))

pm_coords <- do.call(rbind, st_geometry(pm_data2)) %>% as_tibble() 
names(pm_coords) <- c("x", "y")
pm_sp_lonlat <- pm_data2 %>%
  st_transform(crs = ll_wgs84)
pm_coords2 <- do.call(rbind, st_geometry(pm_sp_lonlat)) %>%
  as_tibble() %>% setNames(c("lon","lat"))

pm_sp_cov <- st_set_geometry(pm_data2, NULL) %>%
  dplyr::select(monitor_id) %>%
  rename(ID = monitor_id)
pm_sp_cov <- bind_cols(pm_sp_cov, pm_coords, pm_coords2) %>%
  as.data.frame() %>%
  distinct()
```

Combine the observations and the spatial covariates
Scale the pollutant measurements
```{r}
#' Join log-transformed observations
pol_obs <- left_join(no2_obs, bc_cent_obs, by = "sample_week") %>% 
  left_join(pm_obs, by = "sample_week")
glimpse(pol_obs)

#' Scale the measurements to avoid issues where units are different
#' Remember! These have been log-transformed before scaling
#' Scale the data by columns (monitors)
rownames(pol_obs) <- pol_obs$sample_week
pol_obs$sample_week <- NULL
pol_obs <- as.matrix(pol_obs)

rnames <- rownames(pol_obs)
cnames <- colnames(pol_obs)

pol_obs <- apply(pol_obs, 2, scale)

rownames(pol_obs) <- rnames
colnames(pol_obs) <- cnames

head(rownames(pol_obs))
head(colnames(pol_obs))
class(pol_obs)
dim(pol_obs)

#' SP variables
pol_sp_cov <- bind_rows(no2_sp_cov, bc_cent_sp_cov, pm_sp_cov)
pol_sp_cov
```

Create the data object
```{r}
pol_STdata <- createSTdata(obs = pol_obs,
                           covars = pol_sp_cov)
print(pol_STdata)
```

## Generate the temporal trend 

The cross validation results for generating the time trends suggest 2 basis functions is the way to go, but let's see if that holds up with our data

```{r}
D_pol <- createDataMatrix(pol_STdata)
# D_pol

n_years <- length(2009:2018)
n_years*4
n_years*8

#' Trying 4 df per year
pol.SVD.cv.4py <- SVDsmoothCV(D_pol, 0:4, df = n_years * 4)
print(pol.SVD.cv.4py)
plot(pol.SVD.cv.4py)

#' Trying 8 df per year
pol.SVD.cv.8py <- SVDsmoothCV(D_pol, 0:4, df = n_years * 8)
print(pol.SVD.cv.8py)
plot(pol.SVD.cv.8py)
```

## Plot the time trends

I tried using 1 and 2 basis functions for the time trends. I also checked out 4 and 8 df per year

### 1 basis function
First, see what the time trends for the monitoring data look like with 1 basis function. 

**4 df per year**
```{r}
pol_STdata1 <- updateTrend(pol_STdata, n.basis = 1, df = n_years * 4)
pol_STdata1

head(pol_STdata1$trend)
plot(pol_STdata1$trend$date, pol_STdata1$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
lines(pol_STdata1$trend$date, pol_STdata1$atrend$V1, col = 1)
```

**8 df per year**
```{r}
pol_STdata1a <- updateTrend(pol_STdata, n.basis = 1, df = n_years * 8)
pol_STdata1a

head(pol_STdata1a$trend)
plot(pol_STdata1a$trend$date, pol_STdata1a$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
lines(pol_STdata1a$trend$date, pol_STdata1a$atrend$V1, col = 1)
```

### 2 basis functions
Next, let's see what 2 basis functions look like:

**4 df per year**
```{r}
pol_STdata2 <- updateTrend(pol_STdata, n.basis = 2, df = n_years * 4)
pol_STdata2

head(pol_STdata2$trend)
plot(pol_STdata2$trend$date, pol_STdata2$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
points(pol_STdata2$trend$date, pol_STdata2$trend$V2, col = 2, pch = 16, cex = 0.5)
lines(pol_STdata2$trend$date, pol_STdata2$trend$V1, col = 1)
lines(pol_STdata2$trend$date, pol_STdata2$trend$V2, col = 2)
```

**8 df per year**
```{r}
pol_STdata2a <- updateTrend(pol_STdata, n.basis = 2, df = n_years * 8)
pol_STdata2a

head(pol_STdata2a$trend)
plot(pol_STdata2a$trend$date, pol_STdata2a$trend$V1, col = 1, pch = 16, cex = 0.5,
     xlab = "Date", ylab = "Pollutants (scaled)")
points(pol_STdata2a$trend$date, pol_STdata2a$trend$V2, col = 2, pch = 16, cex = 0.5)
lines(pol_STdata2a$trend$date, pol_STdata2a$trend$V1, col = 1)
lines(pol_STdata2a$trend$date, pol_STdata2a$trend$V2, col = 2)
```

Note that the two basis function plot based on the updated monitoring data set (which includes PM2.5) looks different from when it did with just the NO2 and BC data

## Update the Denver data set

The next step is to update the trend data for the ```denver.data``` object and see if these trend data fit our observations.

Comparing the plots using one basis function vs. two basis functions, it looks like using two basis functions might be better than 1. There was some residual autocorrelation in the central site monitor data when using just one basis function (see the fourth plot in the first series of plots below). Using 8 degrees of freedom did not help.

Using two basis functions helped somewhat to reduce this autocorrelation, but it still persists. There are still some noticable trends in the residuals for the central monitoring site

### 1 basis function, 4 df per year

First update the trend data, then plot the trends for three distributed sites and the central BC monitoring site. 

```{r}
denver.data2.1 <- denver.data
denver.data2.1$trend <- pol_STdata1$trend
print(denver.data2.1)
```

```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.1, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_1")
plot(denver.data2.1, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_10", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.1, "res", ID="d_10", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_10")
plot(denver.data2.1, "pacf", ID="d_10")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.1, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_20")
plot(denver.data2.1, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.1, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="d_53")
plot(denver.data2.1, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.1, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1, "acf", ID="central")
plot(denver.data2.1, "pacf", ID="central")
```

### 1 basis function, 8 df per year

```{r}
denver.data2.1a <- denver.data
denver.data2.1a$trend <- pol_STdata1a$trend
print(denver.data2.1a)
```

```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1a, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.1a, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1a, "acf", ID="d_1")
plot(denver.data2.1a, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1a, "obs", ID="d_10", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.1a, "res", ID="d_10", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1a, "acf", ID="d_10")
plot(denver.data2.1a, "pacf", ID="d_10")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1a, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.1a, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1a, "acf", ID="d_20")
plot(denver.data2.1a, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1a, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.1a, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1a, "acf", ID="d_53")
plot(denver.data2.1a, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.1a, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.1a, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.1a, "acf", ID="central")
plot(denver.data2.1a, "pacf", ID="central")
```

### 2 basis functions, 4 df per year

```{r}
denver.data2.2 <- denver.data
denver.data2.2$trend <- pol_STdata2$trend
print(denver.data2.2)
```

```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.2, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_1")
plot(denver.data2.2, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_10", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_10")
plot(denver.data2.2, "res", ID="d_10", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_10")
plot(denver.data2.2, "pacf", ID="d_10")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.2, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_20")
plot(denver.data2.2, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.2, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="d_53")
plot(denver.data2.2, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.2, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2, "acf", ID="central")
plot(denver.data2.2, "pacf", ID="central")
```

### 2 basis functions, 8 df per year

```{r}
denver.data2.2a <- denver.data
denver.data2.2a$trend <- pol_STdata2a$trend
print(denver.data2.2a)
```

```{r}
par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2a, "obs", ID="d_1", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_1")
plot(denver.data2.2a, "res", ID="d_1", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2a, "acf", ID="d_1")
plot(denver.data2.2a, "pacf", ID="d_1")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2a, "obs", ID="d_10", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_10")
plot(denver.data2.2a, "res", ID="d_10", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2a, "acf", ID="d_10")
plot(denver.data2.2a, "pacf", ID="d_10")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2a, "obs", ID="d_20", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_20")
plot(denver.data2.2a, "res", ID="d_20", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2a, "acf", ID="d_20")
plot(denver.data2.2a, "pacf", ID="d_20")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2a, "obs", ID="d_53", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend d_53")
plot(denver.data2.2a, "res", ID="d_53", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2a, "acf", ID="d_53")
plot(denver.data2.2a, "pacf", ID="d_53")

par(mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
layout(matrix(c(1,1,2,2,3,4), 3, 2, byrow=TRUE))
plot(denver.data2.2a, "obs", ID="central", xlab="", ylab="BC (log ug/m3)",
     main="Temporal trend central")
plot(denver.data2.2a, "res", ID="central", xlab="", ylab="BC (log ug/m3)")
plot(denver.data2.2a, "acf", ID="central")
plot(denver.data2.2a, "pacf", ID="central")
```

# Testing possible models

Building on the results of the previous modeling efforts, I'm going to update "Model 4.2", which used an '```exp``` covariance structure for the error term and ```iid```  for the beta fields.

## Model A: 1 temporal trend (basis) function

### Create the model object
For this version of the model, use ```iid``` for ```cov.beta``` (beta, beta1) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
denver.data.A <- denver.data2.1
names(denver.data.A$covars)

LUR.A <- list(covar_fun, covar_fun)

cov.beta.A <-  list(covf = c("iid", "iid"), nugget = T)
cov.nu.A <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations.A <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.A <- createSTmodel(denver.data.A, LUR = LUR.A,
                                ST = c("bc_st_no2", "bc_st_smk"),
                                cov.beta = cov.beta.A, cov.nu = cov.nu.A,
                                locations = locations.A)
denver.model.A
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.A, all=FALSE)
names

# x.init.A <- cbind(c(0, 0, 0, 0, 0),
#                   c(-1, -1, 0, -1, -1),
#                   c(-1, -1, 0, -5, -1),
#                   c(-5, -5, 0, -1, -5),
#                   c(-5, -5, 0, -5, -5),
#                   c(-1, -1, 2, -1, -1),
#                   c(-1, -1, 2, -5, -1),
#                   c(-5, -5, 2, -1, -5),
#                   c(-5, -5, 2, -5, -5),
#                   c(-1, -1, 4, -1, -1),
#                   c(-1, -1, 4, -5, -1),
#                   c(-5, -5, 4, -1, -5),
#                   c(-5, -5, 4, -5, -5))

x.init.A <- cbind(c(0, 0, 0, 0, 0),
                  c(-1, -1, 4, -5, -1),
                  c(-5, -5, 4, -1, -5),
                  c(-1, -1, 8, -5, -1),
                  c(-5, -5, 8, -1, -5),
                  c(-1, -1, 12, -5, -1),
                  c(-5, -5, 12, -1, -5))

rownames(x.init.A) <- loglikeSTnames(denver.model.A, all=FALSE)
x.init.A

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.A <- estimate.STmodel(denver.model.A, x.init.A)
print(est.denver.model.A)
```

### Cross-validation

Define the CV groups
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.A <- createCV(denver.model.A, groups = 10, #min.dist = .1,
                     subset = paste0("d_", c(1:60)))

ID.cv.A <- sapply(split(denver.model.A$obs$ID, Ind.cv.A), unique)
print(sapply(ID.cv.A, length))
table(Ind.cv.A)

I.col.A <- apply(sapply(ID.cv.A, function(x) denver.model.A$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.A) <- denver.model.A$locations$ID
print(I.col.A)

par(mfrow=c(1,1))
plot(denver.model.A$locations$long,
     denver.model.A$locations$lat,
     pch=23+floor(I.col.A/max(I.col.A)+.5), bg=I.col.A,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.A.cv <- coef(est.denver.model.A, pars="cov")[,c("par","init")]
x.init.A.cv
```

Run the model with cross validation
```{r}
est.denver.A.cv <- estimateCV(denver.model.A, x.init.A.cv, Ind.cv.A)
print(est.denver.A.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.A, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.A.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data.A, denver.model.A, est.denver.model.A, est.denver.A.cv,
     file = here::here("Results", "Denver_ST_Model_A.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.A.cv <- predictCV(denver.model.A, est.denver.A.cv, LTA = T)
pred.A.cv.log <- predictCV(denver.model.A, est.denver.A.cv,
                           LTA = T, transform="unbiased")

summary(pred.A.cv)
summary(pred.A.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.A.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.A.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

# jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_ModA.jpeg"),
#      width = 8, height = 4, units = "in", res = 500)
# par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
# plot(pred.A.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.A.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# dev.off()
```

### Predictions for 2009-2019

What do the long-term predictions look like for this model?
Predicting at the distributed (residential + community) sites

```{r}
x.A <- coef(est.denver.model.A, pars = "cov")$par 
x.A

E.A <- predict(denver.model.A, est.denver.model.A, LTA = T, 
               transform="unbiased", pred.var=FALSE)
print(E.A)

week_preds <- as.data.frame(E.A$EX) %>% 
  mutate(week = as.Date(rownames(E.A$EX)),
         year = year(as.Date(rownames(E.A$EX))))

week_sites <- colnames(week_preds)[which(str_detect(colnames(week_preds), "d_"))]

box_preds <- week_preds %>% 
  select(week, all_of(week_sites)) %>% 
  #filter(week %in% week_weeks) %>% 
  mutate(month = month(week),
         year = year(week)) %>% 
  filter(year %in% c(2009:2019))

box_data <- box_preds %>% 
  pivot_longer(-c(week, month, year), names_to = "location", values_to = "pred")
summary(box_data)

#' Box plot of weekly BC predicted at all distributed sites grouped by month
box_summary <- box_data %>% 
  group_by(month) %>% 
  summarize(median = median(pred, na.rm=T)) %>% 
  arrange(desc(median))
box_summary  

pred_box_plot <- ggplot(box_data) +
  geom_boxplot(aes(x = as.factor(month), y = pred), #, color = as.factor(month)),
               show.legend = F) +
  #scale_color_viridis(name = "Month", discrete = T) +
  facet_wrap(. ~ year, scales = "free") +
  xlab("") + ylab("Distributed site BC concentration (\u03BCg/m\u00B3): Predicted") +
  # theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  scale_x_discrete(labels = str_sub(month.name, 1, 1)) 
pred_box_plot
```

## Model B: 2 temporal trend (basis) functions

### Create the model object
For this version of the model, use ```iid``` for ```cov.beta``` (beta, beta1, beta2) and ```exp``` for ```cov.nu``` (error).
Here we can specify different LUR formluae. The length of the LUR list should be number of basis functions + 1.

```{r}
denver.data.B <- denver.data2.2
names(denver.data.B$covars)

LUR.B <- list(covar_fun, covar_fun, covar_fun)

cov.beta.B <-  list(covf = c("iid", "iid", "iid"), nugget = T)
cov.nu.B <- list(covf = "exp", nugget = T, random.effect = FALSE)
locations.B <- list(coords = c("x","y"), long.lat = c("lon","lat"), others= "type")

denver.model.B <- createSTmodel(denver.data.B, LUR = LUR.B,
                                ST = c("bc_st_no2", "bc_st_smk"),
                                cov.beta = cov.beta.B, cov.nu = cov.nu.B,
                                locations = locations.B)
denver.model.B
```

### Estimate model parameters

- nugget values can range from -1 to -6
- range values can range from 0 to 4
- try starting values where the sill and nugget are the same (e.g., both 0) and where they are very different (e.g., 0 and -5)
- make sure the starting values are pretty different

```{r}
names <- loglikeSTnames(denver.model.B, all=FALSE)
names

# x.init.B <- cbind(c(0, 0, 0, 0, 0, 0),
#                   c(-1, -1, -1, 0, -1, -1),
#                   c(-1, -1, -1, 0, -5, -1),
#                   c(-5, -5, -5, 0, -1, -5),
#                   c(-5, -5, -5, 0, -5, -5),
#                   c(-1, -1, -1, 2, -1, -1),
#                   c(-1, -1, -1, 2, -5, -1),
#                   c(-5, -5, -5, 2, -1, -5),
#                   c(-5, -5, -5, 2, -5, -5),
#                   c(-1, -1, -1, 4, -1, -1),
#                   c(-1, -1, -1, 4, -5, -1),
#                   c(-5, -5, -5, 4, -1, -5),
#                   c(-5, -5, -5, 4, -5, -5))

x.init.B <- cbind(c(0, 0, 0, 0, 0, 0),
                  c(-10, -5, -5, 4, -3, -5),
                  c(-10, -5, -5, 4, -5, -5),
                  c(-10, -5, -5, 6, -3, -5),
                  c(-10, -5, -5, 6, -5, -5),
                  c(-10, -5, -5, 8, -3, -5),
                  c(-10, -5, -5, 8, -5, -5),
                  c(-10, -5, -5, 10, -3, -5),
                  c(-10, -5, -5, 10, -5, -5))

rownames(x.init.B) <- loglikeSTnames(denver.model.B, all=FALSE)
x.init.B

#' Difference from tutorial: use Josh Keller's version of the function
source(here::here("Code", "functions_model.R"))
est.denver.model.B <- estimate.STmodel(denver.model.B, x.init.B)
print(est.denver.model.B)
```

### Cross-validation

Define the CV groups
```{r}
set.seed(123)

unique(colnames(bc_obs2))
Ind.cv.B <- createCV(denver.model.B, groups = 10, #min.dist = .1,
                     subset = paste0("d_", c(1:60)))

ID.cv.B <- sapply(split(denver.model.B$obs$ID, Ind.cv.B), unique)
print(sapply(ID.cv.B, length))
table(Ind.cv.B)

I.col.B <- apply(sapply(ID.cv.B, function(x) denver.model.B$locations$ID%in% x), 1,
                       function(x) if(sum(x)==1) which(x) else 0)
names(I.col.B) <- denver.model.B$locations$ID
print(I.col.B)

par(mfrow=c(1,1))
plot(denver.model.B$locations$long,
     denver.model.B$locations$lat,
     pch=23+floor(I.col.B/max(I.col.B)+.5), bg=I.col.B,
     xlab="Longitude", ylab="Latitude")
map("county", "colorado", col="#FFFF0055",fill=TRUE, add=TRUE)
```

ID starting conditions using previous model without CV:
```{r}
x.init.B.cv <- coef(est.denver.model.B, pars="cov")[,c("par","init")]
x.init.B.cv
```

Run the model with cross validation
```{r}
est.denver.B.cv <- estimateCV(denver.model.B, x.init.B.cv, Ind.cv.B)
print(est.denver.B.cv)

par(mfrow=c(1,1), mar=c(13.5,2.5,.5,.5), las=2)
with(coef(est.denver.model.B, pars="all"),
     plotCI((1:length(par))+.3, par, uiw=1.96*sd,
             col=2, xlab="", xaxt="n", ylab=""))
boxplot(est.denver.B.cv, "all", boxwex=.4, col="grey", add=TRUE)

#' Save the results as an .rdata object
save(denver.data.B, denver.model.B, est.denver.model.B, est.denver.B.cv,
     file = here::here("Results", "Denver_ST_Model_B.rdata"))
```

### Prediction using the CV model
Making predictions using the CV model. Printing out the CV summary statistics as well

```{r}
pred.B.cv <- predictCV(denver.model.B, est.denver.B.cv, LTA = T)
pred.B.cv.log <- predictCV(denver.model.B, est.denver.B.cv,
                           LTA = T, transform="unbiased")

summary(pred.B.cv)
summary(pred.B.cv.log)

par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
plot(pred.B.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
     col=c("ID", "black", "grey"),
     ylim=c(-1,2),
     xlab="Observations", ylab="Predictions",
     main="Cross-validation BC (log ug/m3)")
with(pred.B.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
                                      xlab="Observations", ylab="Predictions",
                                      main="Temporal average BC (ug/m3)"))
abline(0, 1, col="grey")

# jpeg(filename = here::here("Figs", "ST_CV_Obs_vs_Pred_BC_ModA.jpeg"),
#      width = 8, height = 4, units = "in", res = 500)
# par(mfrow=c(1,2), mar=c(3.3,3.3,1.5,1), mgp=c(2,1,0))
# plot(pred.B.cv, "obs", ID="all", pch=c(19,NA), cex=.25, lty=c(NA,2),
#      col=c("ID", "black", "grey"),
#      ylim=c(-1,2),
#      xlab="Observations", ylab="Predictions",
#      main="Cross-validation BC (log ug/m3)")
# with(pred.B.cv.log$pred.LTA, plotCI(obs, EX.pred, uiw=1.96*sqrt(VX.pred),
#                                       xlab="Observations", ylab="Predictions",
#                                       main="Temporal average BC (ug/m3)"))
# abline(0, 1, col="grey")
# dev.off()
```

### Predictions for 2009-2019

What do the long-term predictions look like for this model?
Predicting at the distributed (residential + community) sites

```{r}
x.B <- coef(est.denver.model.B, pars = "cov")$par 
x.B

E.B <- predict(denver.model.B, est.denver.model.B, LTA = T, 
               transform="unbiased", pred.var=FALSE)
print(E.B)

week_preds <- as.data.frame(E.B$EX) %>% 
  mutate(week = as.Date(rownames(E.B$EX)),
         year = year(as.Date(rownames(E.B$EX))))

week_sites <- colnames(week_preds)[which(str_detect(colnames(week_preds), "d_"))]

box_preds <- week_preds %>% 
  select(week, all_of(week_sites)) %>% 
  #filter(week %in% week_weeks) %>% 
  mutate(month = month(week),
         year = year(week)) %>% 
  filter(year %in% c(2009:2019))

box_data <- box_preds %>% 
  pivot_longer(-c(week, month, year), names_to = "location", values_to = "pred")
summary(box_data)

#' Box plot of weekly BC predicted at all distributed sites grouped by month
box_summary <- box_data %>% 
  group_by(month) %>% 
  summarize(median = median(pred, na.rm=T)) %>% 
  arrange(desc(median))
box_summary  

pred_box_plot <- ggplot(box_data) +
  geom_boxplot(aes(x = as.factor(month), y = pred), #, color = as.factor(month)),
               show.legend = F) +
  #scale_color_viridis(name = "Month", discrete = T) +
  facet_wrap(. ~ year, scales = "free") +
  xlab("") + ylab("Distributed site BC concentration (\u03BCg/m\u00B3): Predicted") +
  # theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  scale_x_discrete(labels = str_sub(month.name, 1, 1)) 
pred_box_plot
```


# Summary Table

```{r, include = F, echo = F}

sum_tab <- data.frame(model = c("Model A", "Model B"),
                      trend_data = rep("BC + NO2 + PM2.5", 2),
                      cov_beta0 = c("iid", "iid"),
                      cov_beta1 = c("iid", "iid"),
                      cov_beta2 = c("NA", "iid"),
                      cov_nu_error = c("exp", "exp"),
                      no_basis_fns = c(1, 2),
                      df_per_year = rep(4, 2),
                      all_converge = c(ifelse(exists("est.denver.A.cv"),all(est.denver.A.cv$status$convergence), NA),
                                       ifelse(exists("est.denver.B.cv"),all(est.denver.B.cv$status$convergence), NA)),
                      all_conv = c(ifelse(exists("est.denver.A.cv"),all(est.denver.A.cv$status$conv), NA),
                                   ifelse(exists("est.denver.B.cv"),all(est.denver.B.cv$status$conv), NA)),
                      cv_rmse_obs = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$RMSE[1,3], 2), NA),
                                      ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$RMSE[1,3], 2), NA)),
                      cv_rmse_avg = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$RMSE[2,3], 2), NA),
                                      ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$RMSE[2,3], 2), NA)),
                      cv_r2_obs = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$R2[1,3], 2), NA),
                                    ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$R2[1,3], 2), NA)),
                      cv_r2_avg = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$R2[2,3], 2), NA),
                                    ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$R2[2,3], 2), NA)),
                      cv_coverage_obs = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$coverage[1,1], 2), NA),
                                      ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$coverage[1,1], 2), NA)),
                      cv_coverage_avg = c(ifelse(exists("pred.A.cv.log"), round(summary(pred.A.cv.log)$coverage[2,1], 2), NA),
                                      ifelse(exists("pred.B.cv.log"), round(summary(pred.B.cv.log)$coverage[2,1], 2), NA)))

```

```{r, echo = F, results = "asis"}
library(knitr)
kable(sum_tab, caption = "Summary of model diagnostics") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


<!-- # Additional Notes -->

<!-- ## All of Josh Keller's advice for fitting the intital models -->
<!-- Pasting here for safe keeping -->

<!-- Your sleuthing about that error is correct. A singular hessian matrix (the -->
<!-- hessian is a like a matrix analogue of a second derivative) is symptomatic of -->
<!-- the optimization algorithm not finding the true minimum.  I remember this error -->
<!-- all too well! To deal with this, I suggest the following: -->

<!-- - Use the version of estimate.STmodel() in the attached file instead of -->
<!-- the one that is in the package. If you load the package, then source this file, it will overwrite the package function so you can use this version. This version has two changes: -->

<!--     - It changes the error handling to catch the non-invertible warning -->
<!--     and output an obviously wrong value for the standard deviation (-1000). This doesnt fix the problem at all, but keeps the function from aborting, as the package version does, since the abort makes you lose all of your results that far. This makes it more practical to use multiple starting values in a single function call, since it wont abort half way through. -->
<!--     - It uses the function hessian() from the `numDeriv` package to provide a more numerically-accurate estimate of the Hessian matrix. The package version of the hessian is still stored in the `optim_hessian` object, while the new version is in the `hessian` object.  This means youll need to install the numDeriv package. -->

<!-- - Try a set of different initial conditions. The numerical algorithm used -->
<!-- to maximize the likelihood can get stuck in some areas, so using a set of several initial conditions is a good way to try to ensure that youre getting the best solution. The package should be able to loop through several initial values and pick the one giving the best result. Let me know if youd like any suggested values to try. -->

<!-- - Try a simpler model. Im not sure exactly what model structure youre using, but it often helps to start with a smaller model (e.g. 1 time trend, and iid covariance structure) to get a sense of the ranges of the parameters. You can then use the estimates from the simpler model as approximate starting values for a more complicated fit. -->

